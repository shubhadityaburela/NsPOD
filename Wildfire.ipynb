{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6963bf93-a774-4b7f-b0b4-1ee1ca85e16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from numpy import meshgrid\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from torch.autograd import gradcheck\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import shift\n",
    "from math import sqrt\n",
    "from scipy.sparse import diags\n",
    "from scipy.linalg import cholesky\n",
    "\n",
    "import os\n",
    "inpath = \"./data/Wildfire/Poly/input/\"   # Path for the input data\n",
    "impath = \"./data/Wildfire/Poly/mac/\"\n",
    "immpath = \"./plots/Wildfire/Poly/mac/\"\n",
    "os.makedirs(impath, exist_ok=True)\n",
    "os.makedirs(immpath, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad91d999-b9a0-4eb1-87a5-f83d4a535ff3",
   "metadata": {},
   "source": [
    "# 1D Wildlandfire example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9784ddd8-ffc5-40a7-9129-6489bebf2b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_wf = np.load(inpath + 'SnapShotMatrix558.49.npy', allow_pickle=True)\n",
    "t = np.load(inpath + 'Time.npy', allow_pickle=True)\n",
    "x_grid = np.load(inpath + '1D_Grid.npy', allow_pickle=True)\n",
    "x = x_grid[0]\n",
    "T = Q_wf[:len(x), :]\n",
    "seed = 133"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "667e84f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x109238450>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e32ed120",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = torch.tensor(T/T.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16fa4e08-6af1-4be7-afdf-342abbd873b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Nx = len(x)\n",
    "Nt = len(t)\n",
    "xx, tt = np.meshgrid(x, t)\n",
    "\n",
    "\n",
    "plt.pcolormesh(xx.T, tt.T, Q, cmap='hot')\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"t\")\n",
    "plt.title('Snapshot Matrix Q')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9deb0f-c6ed-4cfc-a618-34dcd83258d6",
   "metadata": {},
   "source": [
    "## Define inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5a7ffe7-d523-4462-aa17-128d9c6d5181",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = np.stack([x.repeat(Nt), np.tile(t, Nx)], axis=1)\n",
    "inputs_tensor = torch.tensor(inputs, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc760045-a001-4f52-aa11-9ba6604714c6",
   "metadata": {},
   "source": [
    "## Define a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf5b954e-d310-4638-a310-0013326f60d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NuclearNormAutograd(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input_matrix):\n",
    "        ctx.save_for_backward(input_matrix)\n",
    "        return torch.linalg.matrix_norm(input_matrix, ord=\"nuc\")\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input_matrix, = ctx.saved_tensors\n",
    "        u, s, v = torch.svd(input_matrix, some=False)\n",
    "        rank = torch.sum(s > 0).item()\n",
    "        dtype = input_matrix.dtype\n",
    "        eye_approx = torch.diag((s > 0).to(dtype)[:rank])\n",
    "        grad_input = torch.matmul(torch.matmul(u[:, :rank], eye_approx), v[:, :rank].t())\n",
    "        return grad_input * grad_output.unsqueeze(-1).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "856059e1-3e8c-418d-b124-d0f5df55b7d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ShapeShiftNet(nn.Module):\n",
    "    def __init__(self, p_init_coeffs1, p_init_coeffs2, p_init_coeffs3):\n",
    "        super(ShapeShiftNet, self).__init__()\n",
    "        \n",
    "        self.alphas1 = nn.ParameterList(\n",
    "            [nn.Parameter(torch.tensor([coeff], dtype=torch.float32), requires_grad=True) for coeff in p_init_coeffs1]\n",
    "        )\n",
    "        self.alphas2 = nn.ParameterList(\n",
    "            [nn.Parameter(torch.tensor([coeff], dtype=torch.float32), requires_grad=True) for coeff in p_init_coeffs2]\n",
    "        )\n",
    "        self.alphas3 = nn.ParameterList(\n",
    "            [nn.Parameter(torch.tensor([coeff], dtype=torch.float32), requires_grad=True) for coeff in p_init_coeffs3]\n",
    "        )\n",
    "        \n",
    "        self.elu = nn.ELU()\n",
    "        \n",
    "        # Subnetwork for f^1\n",
    "        self.f1_fc1 = nn.Linear(2, 5)\n",
    "        self.f1_fc2 = nn.Linear(5, 10)\n",
    "        self.f1_fc3 = nn.Linear(10, 5)\n",
    "        self.f1_fc4 = nn.Linear(5, 1)\n",
    "        \n",
    "        # Subnetwork for f^2\n",
    "        self.f2_fc1 = nn.Linear(2, 5)\n",
    "        self.f2_fc2 = nn.Linear(5, 10)\n",
    "        self.f2_fc3 = nn.Linear(10, 5)\n",
    "        self.f2_fc4 = nn.Linear(5, 1)\n",
    "        \n",
    "        # Subnetwork for f^3\n",
    "        self.f3_fc1 = nn.Linear(2, 5)\n",
    "        self.f3_fc2 = nn.Linear(5, 10)\n",
    "        self.f3_fc3 = nn.Linear(10, 5)\n",
    "        self.f3_fc4 = nn.Linear(5, 1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        # Pathway for f^1 and shift^1\n",
    "        shift1 = sum([coeff * t**(1-i) for i, coeff in enumerate(self.alphas1)])\n",
    "        \n",
    "        x_shifted1 = x + shift1\n",
    "        f1 = self.elu(self.f1_fc1(torch.cat((x_shifted1, t), dim=1)))\n",
    "        f1 = self.elu(self.f1_fc2(f1))\n",
    "        f1 = self.elu(self.f1_fc3(f1))\n",
    "        f1 = self.f1_fc4(f1)\n",
    "        \n",
    "        f1_without_shift = self.elu(self.f1_fc1(torch.cat((x, t), dim=1)))\n",
    "        f1_without_shift = self.elu(self.f1_fc2(f1_without_shift))\n",
    "        f1_without_shift = self.elu(self.f1_fc3(f1_without_shift))\n",
    "        f1_without_shift = self.f1_fc4(f1_without_shift)\n",
    "        \n",
    "        \n",
    "        # Pathway for f^2 and shift^2\n",
    "        shift2 = sum([coeff * t**(1-i) for i, coeff in enumerate(self.alphas2)])\n",
    "        \n",
    "        x_shifted2 = x + shift2\n",
    "        f2 = self.elu(self.f2_fc1(torch.cat((x_shifted2, t), dim=1)))\n",
    "        f2 = self.elu(self.f2_fc2(f2))\n",
    "        f2 = self.elu(self.f2_fc3(f2))\n",
    "        f2 = self.f2_fc4(f2)\n",
    "        \n",
    "        f2_without_shift = self.elu(self.f2_fc1(torch.cat((x, t), dim=1)))\n",
    "        f2_without_shift = self.elu(self.f2_fc2(f2_without_shift))\n",
    "        f2_without_shift = self.elu(self.f2_fc3(f2_without_shift))\n",
    "        f2_without_shift = self.f2_fc4(f2_without_shift)\n",
    "        \n",
    "        \n",
    "        # Pathway for f^3 and shift^3      \n",
    "        shift3 = sum([coeff * t**(1-i) for i, coeff in enumerate(self.alphas3)])\n",
    "        \n",
    "        x_shifted3 = x + shift3\n",
    "        f3 = self.elu(self.f3_fc1(torch.cat((x_shifted3, t), dim=1)))\n",
    "        f3 = self.elu(self.f3_fc2(f3))\n",
    "        f3 = self.elu(self.f3_fc3(f3))\n",
    "        f3 = self.f3_fc4(f3)\n",
    "        \n",
    "        f3_without_shift = self.elu(self.f3_fc1(torch.cat((x, t), dim=1)))\n",
    "        f3_without_shift = self.elu(self.f3_fc2(f3_without_shift))\n",
    "        f3_without_shift = self.elu(self.f3_fc3(f3_without_shift))\n",
    "        f3_without_shift = self.f3_fc4(f3_without_shift)\n",
    "        \n",
    "        return f1, f2, f3, f1_without_shift, f2_without_shift, f3_without_shift\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f7bce08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_fig(filepath, figure=None, **kwargs):\n",
    "    import tikzplotlib\n",
    "    import os\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    ## split extension\n",
    "    fpath = os.path.splitext(filepath)[0]\n",
    "    ## get figure handle\n",
    "    if figure is None:\n",
    "        figure = plt.gcf()\n",
    "    figure.savefig(fpath + \".png\", dpi=300, transparent=True)\n",
    "    tikzplotlib.save(\n",
    "        figure=figure,\n",
    "        filepath=fpath + \".tex\",\n",
    "        axis_height='\\\\figureheight',\n",
    "        axis_width='\\\\figurewidth',\n",
    "        override_externals=True,\n",
    "        **kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc3b6db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_coefficients1 = [1, -1]\n",
    "init_coefficients2 = [0, 0]\n",
    "init_coefficients3 = [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efb55892-854b-44de-b6fa-0160443e759c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = ShapeShiftNet(init_coefficients1, init_coefficients2, init_coefficients3)\n",
    "\n",
    "pretrained_load = True\n",
    "if pretrained_load:\n",
    "    state_dict_original = torch.load(\"./data/Crossing_waves/Poly/seed=54/Crossing_waves.pth\")\n",
    "    state_dict_new = model.state_dict()\n",
    "    \n",
    "    for name, param in state_dict_original.items():\n",
    "        if name in state_dict_new:\n",
    "            state_dict_new[name].copy_(param)  \n",
    "\n",
    "\n",
    "    state_dict_new['alphas1.0'] = torch.tensor([1.5], dtype=torch.float32)  # state_dict_original['alphas2.0']\n",
    "    state_dict_new['alphas1.1'] = torch.tensor([-1], dtype=torch.float32)   # state_dict_original['alphas2.1']\n",
    "    state_dict_new['alphas2.0'] = torch.tensor([0], dtype=torch.float32)   # -state_dict_original['alphas2.0']\n",
    "    state_dict_new['alphas2.1'] = torch.tensor([0], dtype=torch.float32)  # -state_dict_original['alphas2.1']\n",
    "    state_dict_new['alphas3.0'] = torch.tensor([-1.5], dtype=torch.float32)   # -state_dict_original['alphas2.0']\n",
    "    state_dict_new['alphas3.1'] = torch.tensor([1], dtype=torch.float32)  # -state_dict_original['alphas2.1']\n",
    "\n",
    "    \n",
    "#     state_dict_new['f1_fc1.weight'] = state_dict_original['f1_fc1.weight']\n",
    "#     state_dict_new['f1_fc1.bias'] = state_dict_original['f1_fc1.bias']\n",
    "#     state_dict_new['f1_fc2.weight'] = state_dict_original['f1_fc2.weight']\n",
    "#     state_dict_new['f1_fc2.bias'] = state_dict_original['f1_fc2.bias']\n",
    "#     state_dict_new['f1_fc3.weight'] = state_dict_original['f1_fc3.weight']\n",
    "#     state_dict_new['f1_fc3.bias'] = state_dict_original['f1_fc3.bias']\n",
    "#     state_dict_new['f1_fc4.weight'] = state_dict_original['f1_fc4.weight']\n",
    "#     state_dict_new['f1_fc4.bias'] = state_dict_original['f1_fc4.bias']\n",
    "    \n",
    "#     state_dict_new['f2_fc1.weight'] = state_dict_original['f2_fc1.weight']\n",
    "#     state_dict_new['f2_fc1.bias'] = state_dict_original['f2_fc1.bias']\n",
    "#     state_dict_new['f2_fc2.weight'] = state_dict_original['f2_fc2.weight']\n",
    "#     state_dict_new['f2_fc2.bias'] = state_dict_original['f2_fc2.bias']\n",
    "#     state_dict_new['f2_fc3.weight'] = state_dict_original['f2_fc3.weight']\n",
    "#     state_dict_new['f2_fc3.bias'] = state_dict_original['f2_fc3.bias']\n",
    "#     state_dict_new['f2_fc4.weight'] = state_dict_original['f2_fc4.weight']\n",
    "#     state_dict_new['f2_fc4.bias'] = state_dict_original['f2_fc4.bias']\n",
    "    \n",
    "#     state_dict_new['f3_fc1.weight'] = state_dict_original['f3_fc1.weight']\n",
    "#     state_dict_new['f3_fc1.bias'] = state_dict_original['f3_fc1.bias']\n",
    "#     state_dict_new['f3_fc2.weight'] = state_dict_original['f3_fc2.weight']\n",
    "#     state_dict_new['f3_fc2.bias'] = state_dict_original['f3_fc2.bias']\n",
    "#     state_dict_new['f3_fc3.weight'] = state_dict_original['f3_fc3.weight']\n",
    "#     state_dict_new['f3_fc3.bias'] = state_dict_original['f3_fc3.bias']\n",
    "#     state_dict_new['f3_fc4.weight'] = state_dict_original['f3_fc4.weight']\n",
    "#     state_dict_new['f3_fc4.bias'] = state_dict_original['f3_fc4.bias']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    state_dict_new['f1_fc1.weight'] = state_dict_original['f2_fc1.weight']\n",
    "    state_dict_new['f1_fc1.bias'] = state_dict_original['f2_fc1.bias']\n",
    "    state_dict_new['f1_fc2.weight'] = state_dict_original['f2_fc2.weight']\n",
    "    state_dict_new['f1_fc2.bias'] = state_dict_original['f2_fc2.bias']\n",
    "    state_dict_new['f1_fc3.weight'] = state_dict_original['f2_fc3.weight']\n",
    "    state_dict_new['f1_fc3.bias'] = state_dict_original['f2_fc3.bias']\n",
    "    state_dict_new['f1_fc4.weight'] = state_dict_original['f2_fc4.weight']\n",
    "    state_dict_new['f1_fc4.bias'] = state_dict_original['f2_fc4.bias']\n",
    "    \n",
    "    state_dict_new['f2_fc1.weight'] = state_dict_original['f2_fc1.weight']\n",
    "    state_dict_new['f2_fc1.bias'] = state_dict_original['f2_fc1.bias']\n",
    "    state_dict_new['f2_fc2.weight'] = state_dict_original['f2_fc2.weight']\n",
    "    state_dict_new['f2_fc2.bias'] = state_dict_original['f2_fc2.bias']\n",
    "    state_dict_new['f2_fc3.weight'] = state_dict_original['f2_fc3.weight']\n",
    "    state_dict_new['f2_fc3.bias'] = state_dict_original['f2_fc3.bias']\n",
    "    state_dict_new['f2_fc4.weight'] = state_dict_original['f2_fc4.weight']\n",
    "    state_dict_new['f2_fc4.bias'] = state_dict_original['f2_fc4.bias']\n",
    "    \n",
    "    state_dict_new['f3_fc1.weight'] = state_dict_original['f2_fc1.weight']\n",
    "    state_dict_new['f3_fc1.bias'] = state_dict_original['f2_fc1.bias']\n",
    "    state_dict_new['f3_fc2.weight'] = state_dict_original['f2_fc2.weight']\n",
    "    state_dict_new['f3_fc2.bias'] = state_dict_original['f2_fc2.bias']\n",
    "    state_dict_new['f3_fc3.weight'] = state_dict_original['f2_fc3.weight']\n",
    "    state_dict_new['f3_fc3.bias'] = state_dict_original['f2_fc3.bias']\n",
    "    state_dict_new['f3_fc4.weight'] = state_dict_original['f2_fc4.weight']\n",
    "    state_dict_new['f3_fc4.bias'] = state_dict_original['f2_fc4.bias']\n",
    "    \n",
    "    model.load_state_dict(state_dict_new, strict=False)\n",
    "    \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d63593f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TV(Q, Nx, Nt):\n",
    "    \n",
    "    tv_h = torch.pow(Q[:, 1:]-Q[:, :-1], 2).sum()\n",
    "\n",
    "    return (tv_h)/(Nx * Nt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2b8e63-ade1-4af0-be9d-e81742faf497",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/100000, Frob Loss: 66011055.58319209, Nuclear Loss: 891.2388916015625, Total loss: 66011947.101647235,Coefficients_1:tensor([ 1.4999, -0.9999]), Coefficients_2:tensor([-1.0000e-04, -1.0000e-04]), Coefficients_3:tensor([-1.5001,  0.9999])\n",
      "Epoch 100/100000, Frob Loss: 9839215.50831597, Nuclear Loss: 386.2687683105469, Total loss: 9839602.026536707,Coefficients_1:tensor([ 1.4944, -0.9888]), Coefficients_2:tensor([-0.0066, -0.0015]), Coefficients_3:tensor([-1.5076,  0.9928])\n",
      "Epoch 200/100000, Frob Loss: 764776.3791927943, Nuclear Loss: 149.9207763671875, Total loss: 764926.5270978758,Coefficients_1:tensor([ 1.4955, -0.9785]), Coefficients_2:tensor([-0.0075,  0.0086]), Coefficients_3:tensor([-1.5093,  0.9927])\n",
      "Epoch 300/100000, Frob Loss: 195053.16349542746, Nuclear Loss: 88.84087371826172, Total loss: 195142.21276826272,Coefficients_1:tensor([ 1.4972, -0.9692]), Coefficients_2:tensor([-0.0072,  0.0181]), Coefficients_3:tensor([-1.5093,  0.9942])\n",
      "Epoch 400/100000, Frob Loss: 165033.4333156282, Nuclear Loss: 81.05583953857422, Total loss: 165114.68217235152,Coefficients_1:tensor([ 1.4987, -0.9601]), Coefficients_2:tensor([-0.0068,  0.0269]), Coefficients_3:tensor([-1.5093,  0.9952])\n",
      "Epoch 500/100000, Frob Loss: 148398.71326617274, Nuclear Loss: 78.62662506103516, Total loss: 148477.51969906065,Coefficients_1:tensor([ 1.5004, -0.9513]), Coefficients_2:tensor([-0.0064,  0.0354]), Coefficients_3:tensor([-1.5092,  0.9955])\n",
      "Epoch 600/100000, Frob Loss: 134044.12456422998, Nuclear Loss: 76.5163345336914, Total loss: 134120.8090885277,Coefficients_1:tensor([ 1.5021, -0.9429]), Coefficients_2:tensor([-0.0059,  0.0438]), Coefficients_3:tensor([-1.5092,  0.9951])\n",
      "Epoch 700/100000, Frob Loss: 121549.16952113806, Nuclear Loss: 74.64989471435547, Total loss: 121623.9771556258,Coefficients_1:tensor([ 1.5038, -0.9347]), Coefficients_2:tensor([-0.0054,  0.0521]), Coefficients_3:tensor([-1.5092,  0.9941])\n",
      "Epoch 800/100000, Frob Loss: 110641.16940866169, Nuclear Loss: 73.10768127441406, Total loss: 110714.4253001785,Coefficients_1:tensor([ 1.5055, -0.9267]), Coefficients_2:tensor([-0.0049,  0.0603]), Coefficients_3:tensor([-1.5092,  0.9924])\n",
      "Epoch 900/100000, Frob Loss: 100588.45833369586, Nuclear Loss: 72.802978515625, Total loss: 100661.40081884954,Coefficients_1:tensor([ 1.5072, -0.9191]), Coefficients_2:tensor([-0.0044,  0.0683]), Coefficients_3:tensor([-1.5093,  0.9902])\n",
      "Epoch 1000/100000, Frob Loss: 91685.91262771208, Nuclear Loss: 72.37586975097656, Total loss: 91758.42019026,Coefficients_1:tensor([ 1.5088, -0.9117]), Coefficients_2:tensor([-0.0038,  0.0761]), Coefficients_3:tensor([-1.5093,  0.9882])\n",
      "Epoch 1100/100000, Frob Loss: 84142.27761180507, Nuclear Loss: 69.63446044921875, Total loss: 84212.03531291889,Coefficients_1:tensor([ 1.5106, -0.9044]), Coefficients_2:tensor([-0.0033,  0.0834]), Coefficients_3:tensor([-1.5093,  0.9863])\n",
      "Epoch 1200/100000, Frob Loss: 77476.39723301526, Nuclear Loss: 66.0676498413086, Total loss: 77542.58055934902,Coefficients_1:tensor([ 1.5124, -0.8973]), Coefficients_2:tensor([-0.0029,  0.0903]), Coefficients_3:tensor([-1.5093,  0.9840])\n",
      "Epoch 1300/100000, Frob Loss: 71528.35391578468, Nuclear Loss: 64.4527587890625, Total loss: 71592.9155877048,Coefficients_1:tensor([ 1.5143, -0.8903]), Coefficients_2:tensor([-0.0024,  0.0969]), Coefficients_3:tensor([-1.5094,  0.9813])\n",
      "Epoch 1400/100000, Frob Loss: 66120.62958862337, Nuclear Loss: 62.92448043823242, Total loss: 66183.6567728937,Coefficients_1:tensor([ 1.5161, -0.8834]), Coefficients_2:tensor([-0.0019,  0.1035]), Coefficients_3:tensor([-1.5095,  0.9783])\n",
      "Epoch 1500/100000, Frob Loss: 61181.63995855809, Nuclear Loss: 61.23678207397461, Total loss: 61242.97369616241,Coefficients_1:tensor([ 1.5180, -0.8767]), Coefficients_2:tensor([-0.0014,  0.1099]), Coefficients_3:tensor([-1.5096,  0.9748])\n",
      "Epoch 1600/100000, Frob Loss: 56579.499067496065, Nuclear Loss: 58.39040756225586, Total loss: 56637.98087860972,Coefficients_1:tensor([ 1.5199, -0.8701]), Coefficients_2:tensor([-0.0009,  0.1163]), Coefficients_3:tensor([-1.5098,  0.9710])\n",
      "Epoch 1700/100000, Frob Loss: 51833.91651942725, Nuclear Loss: 54.898109436035156, Total loss: 51888.90048278267,Coefficients_1:tensor([ 1.5217, -0.8638]), Coefficients_2:tensor([-0.0004,  0.1230]), Coefficients_3:tensor([-1.5099,  0.9670])\n",
      "Epoch 1800/100000, Frob Loss: 47225.309437738964, Nuclear Loss: 52.75250244140625, Total loss: 47278.14262858988,Coefficients_1:tensor([ 1.5234, -0.8574]), Coefficients_2:tensor([1.2100e-04, 1.3023e-01]), Coefficients_3:tensor([-1.5101,  0.9625])\n",
      "Epoch 1900/100000, Frob Loss: 43581.99231131893, Nuclear Loss: 51.29600143432617, Total loss: 43633.36456656974,Coefficients_1:tensor([ 1.5252, -0.8512]), Coefficients_2:tensor([0.0007, 0.1373]), Coefficients_3:tensor([-1.5102,  0.9574])\n",
      "Epoch 2000/100000, Frob Loss: 40375.9293304285, Nuclear Loss: 50.054927825927734, Total loss: 40426.056398940644,Coefficients_1:tensor([ 1.5271, -0.8452]), Coefficients_2:tensor([0.0013, 0.1444]), Coefficients_3:tensor([-1.5105,  0.9520])\n",
      "Epoch 2100/100000, Frob Loss: 37487.93718635815, Nuclear Loss: 48.967403411865234, Total loss: 37536.97275569814,Coefficients_1:tensor([ 1.5290, -0.8393]), Coefficients_2:tensor([0.0018, 0.1512]), Coefficients_3:tensor([-1.5107,  0.9465])\n",
      "Epoch 2200/100000, Frob Loss: 34829.520750971904, Nuclear Loss: 48.00004577636719, Total loss: 34877.58483776676,Coefficients_1:tensor([ 1.5310, -0.8334]), Coefficients_2:tensor([0.0024, 0.1580]), Coefficients_3:tensor([-1.5110,  0.9408])\n",
      "Epoch 2300/100000, Frob Loss: 32358.898402895884, Nuclear Loss: 47.45699691772461, Total loss: 32406.41540355991,Coefficients_1:tensor([ 1.5329, -0.8277]), Coefficients_2:tensor([0.0030, 0.1646]), Coefficients_3:tensor([-1.5113,  0.9351])\n",
      "Epoch 2400/100000, Frob Loss: 30108.03162616825, Nuclear Loss: 46.89414596557617, Total loss: 30154.982478472946,Coefficients_1:tensor([ 1.5349, -0.8219]), Coefficients_2:tensor([0.0036, 0.1712]), Coefficients_3:tensor([-1.5116,  0.9291])\n",
      "Epoch 2500/100000, Frob Loss: 28018.876212677667, Nuclear Loss: 46.321441650390625, Total loss: 28065.25182689608,Coefficients_1:tensor([ 1.5370, -0.8164]), Coefficients_2:tensor([0.0041, 0.1781]), Coefficients_3:tensor([-1.5119,  0.9231])\n",
      "Epoch 2600/100000, Frob Loss: 26022.28656013494, Nuclear Loss: 46.46104049682617, Total loss: 26068.80035312658,Coefficients_1:tensor([ 1.5390, -0.8111]), Coefficients_2:tensor([0.0047, 0.1851]), Coefficients_3:tensor([-1.5122,  0.9170])\n",
      "Epoch 2700/100000, Frob Loss: 24219.753510890165, Nuclear Loss: 46.49788284301758, Total loss: 24266.30220971034,Coefficients_1:tensor([ 1.5411, -0.8059]), Coefficients_2:tensor([0.0052, 0.1922]), Coefficients_3:tensor([-1.5125,  0.9109])\n",
      "Epoch 2800/100000, Frob Loss: 22503.80907511093, Nuclear Loss: 46.32143020629883, Total loss: 22550.17916342743,Coefficients_1:tensor([ 1.5433, -0.8009]), Coefficients_2:tensor([0.0057, 0.1997]), Coefficients_3:tensor([-1.5129,  0.9044])\n",
      "Epoch 2900/100000, Frob Loss: 20857.252566278665, Nuclear Loss: 46.135433197021484, Total loss: 20903.434709057896,Coefficients_1:tensor([ 1.5455, -0.7960]), Coefficients_2:tensor([0.0062, 0.2075]), Coefficients_3:tensor([-1.5133,  0.8975])\n",
      "Epoch 3000/100000, Frob Loss: 19322.36589613894, Nuclear Loss: 45.90467071533203, Total loss: 19368.315429214035,Coefficients_1:tensor([ 1.5479, -0.7912]), Coefficients_2:tensor([0.0067, 0.2155]), Coefficients_3:tensor([-1.5137,  0.8907])\n",
      "Epoch 3100/100000, Frob Loss: 17925.796788923006, Nuclear Loss: 45.65263748168945, Total loss: 17971.492494768958,Coefficients_1:tensor([ 1.5505, -0.7865]), Coefficients_2:tensor([0.0071, 0.2236]), Coefficients_3:tensor([-1.5140,  0.8838])\n",
      "Epoch 3200/100000, Frob Loss: 16660.68323011605, Nuclear Loss: 45.61930465698242, Total loss: 16706.344076975267,Coefficients_1:tensor([ 1.5531, -0.7819]), Coefficients_2:tensor([0.0075, 0.2317]), Coefficients_3:tensor([-1.5144,  0.8769])\n",
      "Epoch 3300/100000, Frob Loss: 15517.3663074487, Nuclear Loss: 45.54670333862305, Total loss: 15562.952943962171,Coefficients_1:tensor([ 1.5559, -0.7774]), Coefficients_2:tensor([0.0079, 0.2397]), Coefficients_3:tensor([-1.5148,  0.8703])\n",
      "Epoch 3400/100000, Frob Loss: 14465.003845685545, Nuclear Loss: 45.722808837890625, Total loss: 14510.765046463732,Coefficients_1:tensor([ 1.5587, -0.7730]), Coefficients_2:tensor([0.0082, 0.2476]), Coefficients_3:tensor([-1.5152,  0.8640])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3500/100000, Frob Loss: 13499.25697318583, Nuclear Loss: 46.00010681152344, Total loss: 13545.29397078722,Coefficients_1:tensor([ 1.5615, -0.7687]), Coefficients_2:tensor([0.0086, 0.2554]), Coefficients_3:tensor([-1.5155,  0.8579])\n",
      "Epoch 3600/100000, Frob Loss: 12630.86800512218, Nuclear Loss: 46.353912353515625, Total loss: 12677.257344435091,Coefficients_1:tensor([ 1.5642, -0.7646]), Coefficients_2:tensor([0.0089, 0.2631]), Coefficients_3:tensor([-1.5158,  0.8523])\n",
      "Epoch 3700/100000, Frob Loss: 11848.194418389388, Nuclear Loss: 46.71696090698242, Total loss: 11894.945389527866,Coefficients_1:tensor([ 1.5669, -0.7606]), Coefficients_2:tensor([0.0093, 0.2707]), Coefficients_3:tensor([-1.5162,  0.8471])\n",
      "Epoch 3800/100000, Frob Loss: 11136.034035117742, Nuclear Loss: 47.04896545410156, Total loss: 11183.115633030799,Coefficients_1:tensor([ 1.5695, -0.7568]), Coefficients_2:tensor([0.0096, 0.2782]), Coefficients_3:tensor([-1.5164,  0.8424])\n",
      "Epoch 3900/100000, Frob Loss: 10480.05318693909, Nuclear Loss: 47.351219177246094, Total loss: 10527.43570876959,Coefficients_1:tensor([ 1.5720, -0.7531]), Coefficients_2:tensor([0.0100, 0.2855]), Coefficients_3:tensor([-1.5167,  0.8381])\n",
      "Epoch 4000/100000, Frob Loss: 9870.882919733069, Nuclear Loss: 47.628440856933594, Total loss: 9918.541375004343,Coefficients_1:tensor([ 1.5745, -0.7495]), Coefficients_2:tensor([0.0104, 0.2928]), Coefficients_3:tensor([-1.5170,  0.8341])\n",
      "Epoch 4100/100000, Frob Loss: 9303.267478712884, Nuclear Loss: 47.907020568847656, Total loss: 9351.20327229337,Coefficients_1:tensor([ 1.5770, -0.7460]), Coefficients_2:tensor([0.0107, 0.3000]), Coefficients_3:tensor([-1.5172,  0.8305])\n",
      "Epoch 4200/100000, Frob Loss: 8773.88457573496, Nuclear Loss: 48.250221252441406, Total loss: 8822.162377233732,Coefficients_1:tensor([ 1.5793, -0.7426]), Coefficients_2:tensor([0.0111, 0.3071]), Coefficients_3:tensor([-1.5174,  0.8272])\n",
      "Epoch 4300/100000, Frob Loss: 8278.987291198686, Nuclear Loss: 48.68264389038086, Total loss: 8327.696365371808,Coefficients_1:tensor([ 1.5817, -0.7393]), Coefficients_2:tensor([0.0115, 0.3140]), Coefficients_3:tensor([-1.5176,  0.8243])\n",
      "Epoch 4400/100000, Frob Loss: 7806.354518795615, Nuclear Loss: 49.180599212646484, Total loss: 7855.560422379753,Coefficients_1:tensor([ 1.5839, -0.7360]), Coefficients_2:tensor([0.0119, 0.3208]), Coefficients_3:tensor([-1.5178,  0.8217])\n",
      "Epoch 4500/100000, Frob Loss: 7349.024987905054, Nuclear Loss: 50.097328186035156, Total loss: 7399.146596110194,Coefficients_1:tensor([ 1.5861, -0.7328]), Coefficients_2:tensor([0.0123, 0.3275]), Coefficients_3:tensor([-1.5180,  0.8195])\n",
      "Epoch 4600/100000, Frob Loss: 6944.905874184389, Nuclear Loss: 50.893089294433594, Total loss: 6995.822142631803,Coefficients_1:tensor([ 1.5882, -0.7296]), Coefficients_2:tensor([0.0126, 0.3342]), Coefficients_3:tensor([-1.5181,  0.8176])\n",
      "Epoch 4700/100000, Frob Loss: 6565.358822660061, Nuclear Loss: 51.744686126708984, Total loss: 6617.125610552269,Coefficients_1:tensor([ 1.5902, -0.7265]), Coefficients_2:tensor([0.0131, 0.3407]), Coefficients_3:tensor([-1.5183,  0.8158])\n",
      "Epoch 4800/100000, Frob Loss: 6177.530807726212, Nuclear Loss: 53.483314514160156, Total loss: 6231.03518003983,Coefficients_1:tensor([ 1.5927, -0.7233]), Coefficients_2:tensor([0.0135, 0.3471]), Coefficients_3:tensor([-1.5184,  0.8143])\n",
      "Epoch 4900/100000, Frob Loss: 5802.743743771865, Nuclear Loss: 55.477699279785156, Total loss: 5858.241468612283,Coefficients_1:tensor([ 1.5955, -0.7201]), Coefficients_2:tensor([0.0140, 0.3532]), Coefficients_3:tensor([-1.5185,  0.8133])\n",
      "Epoch 5000/100000, Frob Loss: 5448.515478291299, Nuclear Loss: 57.629425048828125, Total loss: 5506.163963074546,Coefficients_1:tensor([ 1.5985, -0.7168]), Coefficients_2:tensor([0.0145, 0.3590]), Coefficients_3:tensor([-1.5185,  0.8125])\n",
      "Epoch 5100/100000, Frob Loss: 5130.1957362455305, Nuclear Loss: 60.01841354370117, Total loss: 5190.232328293192,Coefficients_1:tensor([ 1.6014, -0.7136]), Coefficients_2:tensor([0.0151, 0.3648]), Coefficients_3:tensor([-1.5186,  0.8123])\n",
      "Epoch 5200/100000, Frob Loss: 4854.112298315935, Nuclear Loss: 62.293739318847656, Total loss: 4916.42341502624,Coefficients_1:tensor([ 1.6041, -0.7103]), Coefficients_2:tensor([0.0157, 0.3700]), Coefficients_3:tensor([-1.5186,  0.8123])\n",
      "Epoch 5300/100000, Frob Loss: 4605.023938388213, Nuclear Loss: 64.68611907958984, Total loss: 4669.726713691487,Coefficients_1:tensor([ 1.6066, -0.7071]), Coefficients_2:tensor([0.0164, 0.3755]), Coefficients_3:tensor([-1.5186,  0.8127])\n",
      "Epoch 5400/100000, Frob Loss: 4380.473383291395, Nuclear Loss: 66.87760162353516, Total loss: 4447.366942005934,Coefficients_1:tensor([ 1.6090, -0.7040]), Coefficients_2:tensor([0.0171, 0.3807]), Coefficients_3:tensor([-1.5186,  0.8132])\n",
      "Epoch 5500/100000, Frob Loss: 4179.150163871454, Nuclear Loss: 68.7397689819336, Total loss: 4247.905181411872,Coefficients_1:tensor([ 1.6112, -0.7008]), Coefficients_2:tensor([0.0178, 0.3857]), Coefficients_3:tensor([-1.5187,  0.8135])\n",
      "Epoch 5600/100000, Frob Loss: 3993.26775084315, Nuclear Loss: 70.6570053100586, Total loss: 4063.9394217195936,Coefficients_1:tensor([ 1.6134, -0.6976]), Coefficients_2:tensor([0.0185, 0.3910]), Coefficients_3:tensor([-1.5187,  0.8137])\n",
      "Epoch 5700/100000, Frob Loss: 3821.4985808005613, Nuclear Loss: 72.36214447021484, Total loss: 3893.8748350303676,Coefficients_1:tensor([ 1.6155, -0.6944]), Coefficients_2:tensor([0.0193, 0.3963]), Coefficients_3:tensor([-1.5188,  0.8137])\n",
      "Epoch 5800/100000, Frob Loss: 3664.8139525176225, Nuclear Loss: 73.9424057006836, Total loss: 3738.769901888372,Coefficients_1:tensor([ 1.6175, -0.6913]), Coefficients_2:tensor([0.0201, 0.4014]), Coefficients_3:tensor([-1.5188,  0.8139])\n",
      "Epoch 5900/100000, Frob Loss: 3519.2967362132113, Nuclear Loss: 75.26747131347656, Total loss: 3594.5772553801726,Coefficients_1:tensor([ 1.6195, -0.6881]), Coefficients_2:tensor([0.0209, 0.4066]), Coefficients_3:tensor([-1.5189,  0.8141])\n",
      "Epoch 6000/100000, Frob Loss: 3382.1192509331768, Nuclear Loss: 76.67878723144531, Total loss: 3458.810608575589,Coefficients_1:tensor([ 1.6215, -0.6850]), Coefficients_2:tensor([0.0218, 0.4120]), Coefficients_3:tensor([-1.5190,  0.8140])\n",
      "Epoch 6100/100000, Frob Loss: 3258.9803495556203, Nuclear Loss: 77.6015625, Total loss: 3336.594028778383,Coefficients_1:tensor([ 1.6234, -0.6819]), Coefficients_2:tensor([0.0226, 0.4169]), Coefficients_3:tensor([-1.5190,  0.8144])\n",
      "Epoch 6200/100000, Frob Loss: 3143.087825015229, Nuclear Loss: 78.45497131347656, Total loss: 3221.5544864209787,Coefficients_1:tensor([ 1.6252, -0.6788]), Coefficients_2:tensor([0.0235, 0.4220]), Coefficients_3:tensor([-1.5191,  0.8141])\n",
      "Epoch 6300/100000, Frob Loss: 3031.661975510404, Nuclear Loss: 79.32083892822266, Total loss: 3110.9940935671184,Coefficients_1:tensor([ 1.6271, -0.6757]), Coefficients_2:tensor([0.0243, 0.4275]), Coefficients_3:tensor([-1.5191,  0.8147])\n",
      "Epoch 6400/100000, Frob Loss: 2929.6123501482402, Nuclear Loss: 79.94298553466797, Total loss: 3009.5662192898785,Coefficients_1:tensor([ 1.6288, -0.6727]), Coefficients_2:tensor([0.0252, 0.4327]), Coefficients_3:tensor([-1.5192,  0.8147])\n",
      "Epoch 6500/100000, Frob Loss: 2832.2151796358726, Nuclear Loss: 80.56871032714844, Total loss: 2912.7943991233683,Coefficients_1:tensor([ 1.6306, -0.6698]), Coefficients_2:tensor([0.0262, 0.4379]), Coefficients_3:tensor([-1.5192,  0.8150])\n",
      "Epoch 6600/100000, Frob Loss: 2740.1323596638645, Nuclear Loss: 81.07771301269531, Total loss: 2821.2202027495036,Coefficients_1:tensor([ 1.6322, -0.6668]), Coefficients_2:tensor([0.0271, 0.4429]), Coefficients_3:tensor([-1.5192,  0.8159])\n",
      "Epoch 6700/100000, Frob Loss: 2649.96393093408, Nuclear Loss: 81.67594909667969, Total loss: 2731.6496664400856,Coefficients_1:tensor([ 1.6339, -0.6640]), Coefficients_2:tensor([0.0281, 0.4485]), Coefficients_3:tensor([-1.5192,  0.8161])\n",
      "Epoch 6800/100000, Frob Loss: 2569.7845186512686, Nuclear Loss: 81.96422576904297, Total loss: 2651.758212937578,Coefficients_1:tensor([ 1.6354, -0.6612]), Coefficients_2:tensor([0.0291, 0.4535]), Coefficients_3:tensor([-1.5193,  0.8164])\n",
      "Epoch 6900/100000, Frob Loss: 2483.8365411485893, Nuclear Loss: 82.57221984863281, Total loss: 2566.417864785285,Coefficients_1:tensor([ 1.6370, -0.6585]), Coefficients_2:tensor([0.0302, 0.4592]), Coefficients_3:tensor([-1.5192,  0.8180])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7000/100000, Frob Loss: 2407.3331799524985, Nuclear Loss: 82.8214340209961, Total loss: 2490.163423192609,Coefficients_1:tensor([ 1.6385, -0.6559]), Coefficients_2:tensor([0.0313, 0.4645]), Coefficients_3:tensor([-1.5192,  0.8187])\n",
      "Epoch 7100/100000, Frob Loss: 2331.493079457492, Nuclear Loss: 83.235595703125, Total loss: 2414.737159532555,Coefficients_1:tensor([ 1.6399, -0.6534]), Coefficients_2:tensor([0.0325, 0.4694]), Coefficients_3:tensor([-1.5192,  0.8207])\n",
      "Epoch 7200/100000, Frob Loss: 2257.8887072414464, Nuclear Loss: 83.55992889404297, Total loss: 2341.456812301122,Coefficients_1:tensor([ 1.6414, -0.6508]), Coefficients_2:tensor([0.0338, 0.4751]), Coefficients_3:tensor([-1.5192,  0.8217])\n",
      "Epoch 7300/100000, Frob Loss: 2189.9939641452706, Nuclear Loss: 83.90702056884766, Total loss: 2273.9088881885014,Coefficients_1:tensor([ 1.6427, -0.6483]), Coefficients_2:tensor([0.0350, 0.4804]), Coefficients_3:tensor([-1.5192,  0.8226])\n",
      "Epoch 7400/100000, Frob Loss: 2122.637658487679, Nuclear Loss: 84.49617004394531, Total loss: 2207.1414337052493,Coefficients_1:tensor([ 1.6441, -0.6459]), Coefficients_2:tensor([0.0364, 0.4859]), Coefficients_3:tensor([-1.5192,  0.8249])\n",
      "Epoch 7500/100000, Frob Loss: 2062.0466543841703, Nuclear Loss: 84.8638916015625, Total loss: 2146.917880845309,Coefficients_1:tensor([ 1.6455, -0.6435]), Coefficients_2:tensor([0.0378, 0.4913]), Coefficients_3:tensor([-1.5192,  0.8263])\n",
      "Epoch 7600/100000, Frob Loss: 2004.0144431010858, Nuclear Loss: 85.27754974365234, Total loss: 2089.299060372826,Coefficients_1:tensor([ 1.6470, -0.6413]), Coefficients_2:tensor([0.0393, 0.4969]), Coefficients_3:tensor([-1.5192,  0.8279])\n",
      "Epoch 7700/100000, Frob Loss: 1949.1706421809422, Nuclear Loss: 85.59961700439453, Total loss: 2034.7770768288674,Coefficients_1:tensor([ 1.6484, -0.6392]), Coefficients_2:tensor([0.0407, 0.5021]), Coefficients_3:tensor([-1.5191,  0.8308])\n",
      "Epoch 7800/100000, Frob Loss: 1896.1227221619295, Nuclear Loss: 85.80375671386719, Total loss: 1981.9330636053303,Coefficients_1:tensor([ 1.6500, -0.6372]), Coefficients_2:tensor([0.0422, 0.5077]), Coefficients_3:tensor([-1.5191,  0.8325])\n",
      "Epoch 7900/100000, Frob Loss: 1846.3916050431867, Nuclear Loss: 85.84681701660156, Total loss: 1932.2447757924524,Coefficients_1:tensor([ 1.6516, -0.6353]), Coefficients_2:tensor([0.0436, 0.5128]), Coefficients_3:tensor([-1.5190,  0.8343])\n",
      "Epoch 8000/100000, Frob Loss: 1788.9945747619015, Nuclear Loss: 85.9304428100586, Total loss: 1874.9312134989989,Coefficients_1:tensor([ 1.6534, -0.6338]), Coefficients_2:tensor([0.0450, 0.5172]), Coefficients_3:tensor([-1.5190,  0.8365])\n",
      "Epoch 8100/100000, Frob Loss: 1720.7847107845898, Nuclear Loss: 86.07842254638672, Total loss: 1806.869163838828,Coefficients_1:tensor([ 1.6555, -0.6326]), Coefficients_2:tensor([0.0467, 0.5224]), Coefficients_3:tensor([-1.5189,  0.8390])\n",
      "Epoch 8200/100000, Frob Loss: 1655.9731977736112, Nuclear Loss: 86.13541412353516, Total loss: 1742.1144342874643,Coefficients_1:tensor([ 1.6580, -0.6315]), Coefficients_2:tensor([0.0487, 0.5284]), Coefficients_3:tensor([-1.5188,  0.8429])\n",
      "Epoch 8300/100000, Frob Loss: 1608.6074515437351, Nuclear Loss: 85.55619049072266, Total loss: 1694.1692524464445,Coefficients_1:tensor([ 1.6601, -0.6304]), Coefficients_2:tensor([0.0503, 0.5326]), Coefficients_3:tensor([-1.5187,  0.8471])\n",
      "Epoch 8400/100000, Frob Loss: 1572.237133468571, Nuclear Loss: 84.86907958984375, Total loss: 1657.1116342270514,Coefficients_1:tensor([ 1.6617, -0.6295]), Coefficients_2:tensor([0.0517, 0.5366]), Coefficients_3:tensor([-1.5186,  0.8503])\n",
      "Epoch 8500/100000, Frob Loss: 1539.3521902843797, Nuclear Loss: 84.30876159667969, Total loss: 1623.666187413358,Coefficients_1:tensor([ 1.6631, -0.6287]), Coefficients_2:tensor([0.0534, 0.5418]), Coefficients_3:tensor([-1.5185,  0.8533])\n",
      "Epoch 8600/100000, Frob Loss: 1511.6770386757244, Nuclear Loss: 83.53368377685547, Total loss: 1595.2157684304666,Coefficients_1:tensor([ 1.6642, -0.6280]), Coefficients_2:tensor([0.0548, 0.5469]), Coefficients_3:tensor([-1.5183,  0.8561])\n",
      "Epoch 8700/100000, Frob Loss: 1488.5310065154224, Nuclear Loss: 82.56902313232422, Total loss: 1571.104896007502,Coefficients_1:tensor([ 1.6652, -0.6275]), Coefficients_2:tensor([0.0559, 0.5507]), Coefficients_3:tensor([-1.5181,  0.8592])\n",
      "Epoch 8800/100000, Frob Loss: 1466.8802506478685, Nuclear Loss: 81.70186614990234, Total loss: 1548.5868291111844,Coefficients_1:tensor([ 1.6662, -0.6270]), Coefficients_2:tensor([0.0570, 0.5537]), Coefficients_3:tensor([-1.5179,  0.8639])\n",
      "Epoch 8900/100000, Frob Loss: 1446.5923417925205, Nuclear Loss: 80.75865936279297, Total loss: 1527.355554495689,Coefficients_1:tensor([ 1.6672, -0.6264]), Coefficients_2:tensor([0.0581, 0.5568]), Coefficients_3:tensor([-1.5177,  0.8682])\n",
      "Epoch 9000/100000, Frob Loss: 1428.9805628855072, Nuclear Loss: 79.7032241821289, Total loss: 1508.6882010253437,Coefficients_1:tensor([ 1.6680, -0.6260]), Coefficients_2:tensor([0.0591, 0.5600]), Coefficients_3:tensor([-1.5176,  0.8710])\n",
      "Epoch 9100/100000, Frob Loss: 1412.4482734138587, Nuclear Loss: 78.68033599853516, Total loss: 1491.1328969812462,Coefficients_1:tensor([ 1.6688, -0.6257]), Coefficients_2:tensor([0.0602, 0.5637]), Coefficients_3:tensor([-1.5174,  0.8735])\n",
      "Epoch 9200/100000, Frob Loss: 1397.521191527217, Nuclear Loss: 77.53841400146484, Total loss: 1475.0637753816204,Coefficients_1:tensor([ 1.6695, -0.6256]), Coefficients_2:tensor([0.0612, 0.5672]), Coefficients_3:tensor([-1.5173,  0.8761])\n",
      "Epoch 9300/100000, Frob Loss: 1383.2472870286688, Nuclear Loss: 76.41226959228516, Total loss: 1459.6636149978399,Coefficients_1:tensor([ 1.6701, -0.6255]), Coefficients_2:tensor([0.0623, 0.5714]), Coefficients_3:tensor([-1.5171,  0.8793])\n",
      "Epoch 9400/100000, Frob Loss: 1369.4220550598998, Nuclear Loss: 75.27302551269531, Total loss: 1444.6990246072833,Coefficients_1:tensor([ 1.6707, -0.6254]), Coefficients_2:tensor([0.0634, 0.5762]), Coefficients_3:tensor([-1.5169,  0.8833])\n",
      "Epoch 9500/100000, Frob Loss: 1356.679286292565, Nuclear Loss: 74.08711242675781, Total loss: 1430.770242186074,Coefficients_1:tensor([ 1.6714, -0.6252]), Coefficients_2:tensor([0.0643, 0.5801]), Coefficients_3:tensor([-1.5166,  0.8886])\n",
      "Epoch 9600/100000, Frob Loss: 1346.5957631806648, Nuclear Loss: 72.74612426757812, Total loss: 1419.3456407597093,Coefficients_1:tensor([ 1.6721, -0.6250]), Coefficients_2:tensor([0.0653, 0.5836]), Coefficients_3:tensor([-1.5165,  0.8916])\n",
      "Epoch 9700/100000, Frob Loss: 1337.6250639008842, Nuclear Loss: 71.26171112060547, Total loss: 1408.8904465581775,Coefficients_1:tensor([ 1.6726, -0.6249]), Coefficients_2:tensor([0.0663, 0.5867]), Coefficients_3:tensor([-1.5163,  0.8943])\n",
      "Epoch 9800/100000, Frob Loss: 1329.5582251356602, Nuclear Loss: 69.71183013916016, Total loss: 1399.2736495473575,Coefficients_1:tensor([ 1.6732, -0.6249]), Coefficients_2:tensor([0.0671, 0.5893]), Coefficients_3:tensor([-1.5161,  0.8969])\n",
      "Epoch 9900/100000, Frob Loss: 1322.2443948448822, Nuclear Loss: 68.15199279785156, Total loss: 1390.399909551274,Coefficients_1:tensor([ 1.6737, -0.6249]), Coefficients_2:tensor([0.0680, 0.5918]), Coefficients_3:tensor([-1.5159,  0.8994])\n",
      "Epoch 10000/100000, Frob Loss: 1315.4876994737485, Nuclear Loss: 66.56523895263672, Total loss: 1382.0563898147145,Coefficients_1:tensor([ 1.6742, -0.6250]), Coefficients_2:tensor([0.0688, 0.5940]), Coefficients_3:tensor([-1.5157,  0.9020])\n",
      "Epoch 10100/100000, Frob Loss: 1309.2474781951826, Nuclear Loss: 64.9682846069336, Total loss: 1374.2191471317276,Coefficients_1:tensor([ 1.6747, -0.6252]), Coefficients_2:tensor([0.0696, 0.5962]), Coefficients_3:tensor([-1.5154,  0.9047])\n",
      "Epoch 10200/100000, Frob Loss: 1303.4787704890462, Nuclear Loss: 63.39008712768555, Total loss: 1366.872179794764,Coefficients_1:tensor([ 1.6751, -0.6253]), Coefficients_2:tensor([0.0705, 0.5984]), Coefficients_3:tensor([-1.5152,  0.9073])\n",
      "Epoch 10300/100000, Frob Loss: 1298.0177896079363, Nuclear Loss: 61.79714584350586, Total loss: 1359.8181976879237,Coefficients_1:tensor([ 1.6755, -0.6255]), Coefficients_2:tensor([0.0714, 0.6007]), Coefficients_3:tensor([-1.5149,  0.9098])\n",
      "Epoch 10400/100000, Frob Loss: 1292.6721286287584, Nuclear Loss: 60.26753616333008, Total loss: 1352.942873723624,Coefficients_1:tensor([ 1.6759, -0.6257]), Coefficients_2:tensor([0.0724, 0.6036]), Coefficients_3:tensor([-1.5146,  0.9123])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10500/100000, Frob Loss: 1287.3475490846372, Nuclear Loss: 58.82064437866211, Total loss: 1346.1713493271432,Coefficients_1:tensor([ 1.6764, -0.6259]), Coefficients_2:tensor([0.0737, 0.6073]), Coefficients_3:tensor([-1.5143,  0.9150])\n",
      "Epoch 10600/100000, Frob Loss: 1281.5915441390669, Nuclear Loss: 57.45195388793945, Total loss: 1339.0465996778646,Coefficients_1:tensor([ 1.6769, -0.6260]), Coefficients_2:tensor([0.0752, 0.6120]), Coefficients_3:tensor([-1.5140,  0.9178])\n",
      "Epoch 10700/100000, Frob Loss: 1277.0343649523381, Nuclear Loss: 55.87175369262695, Total loss: 1332.9091723394629,Coefficients_1:tensor([ 1.6773, -0.6261]), Coefficients_2:tensor([0.0764, 0.6154]), Coefficients_3:tensor([-1.5137,  0.9207])\n",
      "Epoch 10800/100000, Frob Loss: 1273.21717005609, Nuclear Loss: 54.286678314208984, Total loss: 1327.5068582013662,Coefficients_1:tensor([ 1.6776, -0.6262]), Coefficients_2:tensor([0.0773, 0.6176]), Coefficients_3:tensor([-1.5133,  0.9238])\n",
      "Epoch 10900/100000, Frob Loss: 1269.666909271098, Nuclear Loss: 52.84381866455078, Total loss: 1322.5136956558015,Coefficients_1:tensor([ 1.6778, -0.6263]), Coefficients_2:tensor([0.0781, 0.6192]), Coefficients_3:tensor([-1.5129,  0.9271])\n",
      "Epoch 11000/100000, Frob Loss: 1266.262381667902, Nuclear Loss: 51.626495361328125, Total loss: 1317.89180255139,Coefficients_1:tensor([ 1.6780, -0.6264]), Coefficients_2:tensor([0.0790, 0.6205]), Coefficients_3:tensor([-1.5124,  0.9306])\n",
      "Epoch 11100/100000, Frob Loss: 1262.923397990512, Nuclear Loss: 50.779598236083984, Total loss: 1313.7058806164287,Coefficients_1:tensor([ 1.6780, -0.6266]), Coefficients_2:tensor([0.0799, 0.6216]), Coefficients_3:tensor([-1.5120,  0.9342])\n",
      "Epoch 11200/100000, Frob Loss: 1259.5765284463785, Nuclear Loss: 50.17982864379883, Total loss: 1309.7592032513137,Coefficients_1:tensor([ 1.6779, -0.6266]), Coefficients_2:tensor([0.0809, 0.6223]), Coefficients_3:tensor([-1.5115,  0.9380])\n",
      "Epoch 11300/100000, Frob Loss: 1256.178106043327, Nuclear Loss: 49.65340042114258, Total loss: 1305.8343200176744,Coefficients_1:tensor([ 1.6777, -0.6267]), Coefficients_2:tensor([0.0818, 0.6229]), Coefficients_3:tensor([-1.5110,  0.9421])\n",
      "Epoch 11400/100000, Frob Loss: 1252.8686633970124, Nuclear Loss: 49.151912689208984, Total loss: 1302.0233648289927,Coefficients_1:tensor([ 1.6774, -0.6268]), Coefficients_2:tensor([0.0828, 0.6235]), Coefficients_3:tensor([-1.5105,  0.9465])\n",
      "Epoch 11500/100000, Frob Loss: 1249.681393446558, Nuclear Loss: 48.67182540893555, Total loss: 1298.355986801599,Coefficients_1:tensor([ 1.6771, -0.6268]), Coefficients_2:tensor([0.0837, 0.6242]), Coefficients_3:tensor([-1.5100,  0.9512])\n",
      "Epoch 11600/100000, Frob Loss: 1246.6260260806998, Nuclear Loss: 48.217830657958984, Total loss: 1294.8466065558719,Coefficients_1:tensor([ 1.6769, -0.6267]), Coefficients_2:tensor([0.0848, 0.6248]), Coefficients_3:tensor([-1.5094,  0.9561])\n",
      "Epoch 11700/100000, Frob Loss: 1243.7833228432235, Nuclear Loss: 47.713294982910156, Total loss: 1291.4993506085254,Coefficients_1:tensor([ 1.6766, -0.6265]), Coefficients_2:tensor([0.0858, 0.6252]), Coefficients_3:tensor([-1.5089,  0.9613])\n",
      "Epoch 11800/100000, Frob Loss: 1240.9565131948727, Nuclear Loss: 47.10325241088867, Total loss: 1288.0624822054922,Coefficients_1:tensor([ 1.6764, -0.6261]), Coefficients_2:tensor([0.0870, 0.6254]), Coefficients_3:tensor([-1.5084,  0.9675])\n",
      "Epoch 11900/100000, Frob Loss: 1238.0272086799328, Nuclear Loss: 46.271358489990234, Total loss: 1284.301259523369,Coefficients_1:tensor([ 1.6761, -0.6257]), Coefficients_2:tensor([0.0882, 0.6256]), Coefficients_3:tensor([-1.5082,  0.9741])\n",
      "Epoch 12000/100000, Frob Loss: 1235.6907298026879, Nuclear Loss: 45.577430725097656, Total loss: 1281.270832529505,Coefficients_1:tensor([ 1.6759, -0.6252]), Coefficients_2:tensor([0.0896, 0.6262]), Coefficients_3:tensor([-1.5078,  0.9790])\n",
      "Epoch 12100/100000, Frob Loss: 1233.5949367144008, Nuclear Loss: 45.00284957885742, Total loss: 1278.6004369597745,Coefficients_1:tensor([ 1.6756, -0.6246]), Coefficients_2:tensor([0.0911, 0.6274]), Coefficients_3:tensor([-1.5073,  0.9831])\n",
      "Epoch 12200/100000, Frob Loss: 1231.4020079425604, Nuclear Loss: 44.441654205322266, Total loss: 1275.8462905685951,Coefficients_1:tensor([ 1.6753, -0.6239]), Coefficients_2:tensor([0.0925, 0.6281]), Coefficients_3:tensor([-1.5068,  0.9870])\n",
      "Epoch 12300/100000, Frob Loss: 1229.4103738517072, Nuclear Loss: 43.910579681396484, Total loss: 1273.3235598598178,Coefficients_1:tensor([ 1.6750, -0.6231]), Coefficients_2:tensor([0.0941, 0.6293]), Coefficients_3:tensor([-1.5063,  0.9908])\n",
      "Epoch 12400/100000, Frob Loss: 1227.5219318463987, Nuclear Loss: 43.41620635986328, Total loss: 1270.9407227281536,Coefficients_1:tensor([ 1.6746, -0.6223]), Coefficients_2:tensor([0.0957, 0.6305]), Coefficients_3:tensor([-1.5058,  0.9944])\n",
      "Epoch 12500/100000, Frob Loss: 1225.5815398269608, Nuclear Loss: 42.95490646362305, Total loss: 1268.5390075696907,Coefficients_1:tensor([ 1.6742, -0.6214]), Coefficients_2:tensor([0.0973, 0.6316]), Coefficients_3:tensor([-1.5053,  0.9981])\n",
      "Epoch 12600/100000, Frob Loss: 1223.738063597598, Nuclear Loss: 42.478118896484375, Total loss: 1266.218718829344,Coefficients_1:tensor([ 1.6738, -0.6205]), Coefficients_2:tensor([0.0989, 0.6328]), Coefficients_3:tensor([-1.5048,  1.0017])\n",
      "Epoch 12700/100000, Frob Loss: 1221.9203172122336, Nuclear Loss: 41.96541976928711, Total loss: 1263.888247635795,Coefficients_1:tensor([ 1.6734, -0.6195]), Coefficients_2:tensor([0.1006, 0.6340]), Coefficients_3:tensor([-1.5043,  1.0053])\n",
      "Epoch 12800/100000, Frob Loss: 1220.0254754688942, Nuclear Loss: 41.411014556884766, Total loss: 1261.4389744132643,Coefficients_1:tensor([ 1.6730, -0.6184]), Coefficients_2:tensor([0.1025, 0.6348]), Coefficients_3:tensor([-1.5037,  1.0088])\n",
      "Epoch 12900/100000, Frob Loss: 1218.202497637543, Nuclear Loss: 40.83481979370117, Total loss: 1259.0397751256276,Coefficients_1:tensor([ 1.6726, -0.6174]), Coefficients_2:tensor([0.1044, 0.6360]), Coefficients_3:tensor([-1.5032,  1.0124])\n",
      "Epoch 13000/100000, Frob Loss: 1216.3956134601099, Nuclear Loss: 40.2717170715332, Total loss: 1256.6697614807704,Coefficients_1:tensor([ 1.6721, -0.6163]), Coefficients_2:tensor([0.1065, 0.6376]), Coefficients_3:tensor([-1.5026,  1.0161])\n",
      "Epoch 13100/100000, Frob Loss: 1214.4568655034327, Nuclear Loss: 39.682979583740234, Total loss: 1254.1422499657874,Coefficients_1:tensor([ 1.6716, -0.6152]), Coefficients_2:tensor([0.1087, 0.6393]), Coefficients_3:tensor([-1.5021,  1.0199])\n",
      "Epoch 13200/100000, Frob Loss: 1212.5174081584123, Nuclear Loss: 38.955162048339844, Total loss: 1251.4749462767654,Coefficients_1:tensor([ 1.6711, -0.6141]), Coefficients_2:tensor([0.1111, 0.6419]), Coefficients_3:tensor([-1.5016,  1.0236])\n",
      "Epoch 13300/100000, Frob Loss: 1210.060443106631, Nuclear Loss: 38.30820846557617, Total loss: 1248.371000003826,Coefficients_1:tensor([ 1.6706, -0.6129]), Coefficients_2:tensor([0.1143, 0.6477]), Coefficients_3:tensor([-1.5011,  1.0274])\n",
      "Epoch 13400/100000, Frob Loss: 1207.7818931652305, Nuclear Loss: 37.59817123413086, Total loss: 1245.3823818733522,Coefficients_1:tensor([ 1.6700, -0.6117]), Coefficients_2:tensor([0.1173, 0.6534]), Coefficients_3:tensor([-1.5006,  1.0313])\n",
      "Epoch 13500/100000, Frob Loss: 1204.665174587908, Nuclear Loss: 36.889949798583984, Total loss: 1241.5574060648669,Coefficients_1:tensor([ 1.6694, -0.6105]), Coefficients_2:tensor([0.1199, 0.6570]), Coefficients_3:tensor([-1.5001,  1.0354])\n",
      "Epoch 13600/100000, Frob Loss: 1182.5788436145938, Nuclear Loss: 37.10663986206055, Total loss: 1219.6879226875442,Coefficients_1:tensor([ 1.6705, -0.6104]), Coefficients_2:tensor([0.1223, 0.6595]), Coefficients_3:tensor([-1.4998,  1.0385])\n",
      "Epoch 13700/100000, Frob Loss: 1176.848157579325, Nuclear Loss: 36.60803985595703, Total loss: 1213.458610751912,Coefficients_1:tensor([ 1.6713, -0.6089]), Coefficients_2:tensor([0.1245, 0.6605]), Coefficients_3:tensor([-1.4999,  1.0435])\n",
      "Epoch 13800/100000, Frob Loss: 1172.9490429367145, Nuclear Loss: 36.07944107055664, Total loss: 1209.0308735002038,Coefficients_1:tensor([ 1.6716, -0.6076]), Coefficients_2:tensor([0.1269, 0.6616]), Coefficients_3:tensor([-1.4998,  1.0492])\n",
      "Epoch 13900/100000, Frob Loss: 1169.9013799301101, Nuclear Loss: 35.546993255615234, Total loss: 1205.450732907535,Coefficients_1:tensor([ 1.6713, -0.6060]), Coefficients_2:tensor([0.1296, 0.6640]), Coefficients_3:tensor([-1.4994,  1.0544])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14000/100000, Frob Loss: 1167.1249488250846, Nuclear Loss: 34.962974548339844, Total loss: 1202.0902509890525,Coefficients_1:tensor([ 1.6708, -0.6044]), Coefficients_2:tensor([0.1331, 0.6694]), Coefficients_3:tensor([-1.4990,  1.0594])\n",
      "Epoch 14100/100000, Frob Loss: 1164.6696138207626, Nuclear Loss: 34.37691879272461, Total loss: 1199.048829385807,Coefficients_1:tensor([ 1.6700, -0.6026]), Coefficients_2:tensor([0.1364, 0.6744]), Coefficients_3:tensor([-1.4986,  1.0645])\n",
      "Epoch 14200/100000, Frob Loss: 1162.5082955085397, Nuclear Loss: 33.79025650024414, Total loss: 1196.3008164781802,Coefficients_1:tensor([ 1.6690, -0.6007]), Coefficients_2:tensor([0.1393, 0.6777]), Coefficients_3:tensor([-1.4981,  1.0696])\n",
      "Epoch 14300/100000, Frob Loss: 1160.5343395828943, Nuclear Loss: 33.24552536010742, Total loss: 1193.7820954181927,Coefficients_1:tensor([ 1.6678, -0.5986]), Coefficients_2:tensor([0.1419, 0.6795]), Coefficients_3:tensor([-1.4977,  1.0746])\n",
      "Epoch 14400/100000, Frob Loss: 1158.7775293594261, Nuclear Loss: 32.646602630615234, Total loss: 1191.4263262910338,Coefficients_1:tensor([ 1.6666, -0.5966]), Coefficients_2:tensor([0.1443, 0.6808]), Coefficients_3:tensor([-1.4972,  1.0797])\n",
      "Epoch 14500/100000, Frob Loss: 1157.216650736721, Nuclear Loss: 32.21445846557617, Total loss: 1189.4332736453207,Coefficients_1:tensor([ 1.6658, -0.5951]), Coefficients_2:tensor([0.1463, 0.6815]), Coefficients_3:tensor([-1.4968,  1.0846])\n",
      "Epoch 14600/100000, Frob Loss: 1151.7553702337323, Nuclear Loss: 31.957006454467773, Total loss: 1183.714502618241,Coefficients_1:tensor([ 1.6649, -0.5936]), Coefficients_2:tensor([0.1479, 0.6818]), Coefficients_3:tensor([-1.4963,  1.0895])\n",
      "Epoch 14700/100000, Frob Loss: 1150.5917575627195, Nuclear Loss: 31.62394142150879, Total loss: 1182.2177870932596,Coefficients_1:tensor([ 1.6639, -0.5921]), Coefficients_2:tensor([0.1492, 0.6820]), Coefficients_3:tensor([-1.4959,  1.0943])\n",
      "Epoch 14800/100000, Frob Loss: 1151.3047298553781, Nuclear Loss: 31.305944442749023, Total loss: 1182.6127224091822,Coefficients_1:tensor([ 1.6631, -0.5907]), Coefficients_2:tensor([0.1502, 0.6817]), Coefficients_3:tensor([-1.4955,  1.0992])\n",
      "Epoch 14900/100000, Frob Loss: 1145.568237507416, Nuclear Loss: 30.752172470092773, Total loss: 1176.3224178000148,Coefficients_1:tensor([ 1.6625, -0.5894]), Coefficients_2:tensor([0.1512, 0.6808]), Coefficients_3:tensor([-1.4950,  1.1043])\n",
      "Epoch 15000/100000, Frob Loss: 1143.7222316431935, Nuclear Loss: 30.35395622253418, Total loss: 1174.0781557759392,Coefficients_1:tensor([ 1.6619, -0.5881]), Coefficients_2:tensor([0.1520, 0.6800]), Coefficients_3:tensor([-1.4946,  1.1095])\n",
      "Epoch 15100/100000, Frob Loss: 1141.5138783917876, Nuclear Loss: 30.061737060546875, Total loss: 1171.577545611269,Coefficients_1:tensor([ 1.6615, -0.5869]), Coefficients_2:tensor([0.1527, 0.6791]), Coefficients_3:tensor([-1.4942,  1.1146])\n",
      "Epoch 15200/100000, Frob Loss: 1139.3793921138526, Nuclear Loss: 29.7274227142334, Total loss: 1169.1087100376276,Coefficients_1:tensor([ 1.6609, -0.5856]), Coefficients_2:tensor([0.1534, 0.6786]), Coefficients_3:tensor([-1.4938,  1.1200])\n",
      "Epoch 15300/100000, Frob Loss: 1137.2851679991468, Nuclear Loss: 29.437152862548828, Total loss: 1166.7241864011626,Coefficients_1:tensor([ 1.6604, -0.5842]), Coefficients_2:tensor([0.1541, 0.6787]), Coefficients_3:tensor([-1.4935,  1.1262])\n",
      "Epoch 15400/100000, Frob Loss: 1134.7123899605472, Nuclear Loss: 29.080442428588867, Total loss: 1163.7946726580963,Coefficients_1:tensor([ 1.6599, -0.5827]), Coefficients_2:tensor([0.1548, 0.6793]), Coefficients_3:tensor([-1.4933,  1.1323])\n",
      "Epoch 15500/100000, Frob Loss: 1132.1957234943093, Nuclear Loss: 28.761518478393555, Total loss: 1160.9590587193652,Coefficients_1:tensor([ 1.6594, -0.5812]), Coefficients_2:tensor([0.1554, 0.6801]), Coefficients_3:tensor([-1.4930,  1.1383])\n",
      "Epoch 15600/100000, Frob Loss: 1129.4466444829457, Nuclear Loss: 28.517587661743164, Total loss: 1157.9660305391742,Coefficients_1:tensor([ 1.6589, -0.5797]), Coefficients_2:tensor([0.1560, 0.6807]), Coefficients_3:tensor([-1.4927,  1.1440])\n",
      "Epoch 15700/100000, Frob Loss: 1126.5439174058265, Nuclear Loss: 28.275009155273438, Total loss: 1154.82071030495,Coefficients_1:tensor([ 1.6585, -0.5783]), Coefficients_2:tensor([0.1565, 0.6811]), Coefficients_3:tensor([-1.4924,  1.1487])\n",
      "Epoch 15800/100000, Frob Loss: 1123.7219570955738, Nuclear Loss: 27.99419593811035, Total loss: 1151.7179218376066,Coefficients_1:tensor([ 1.6580, -0.5768]), Coefficients_2:tensor([0.1570, 0.6815]), Coefficients_3:tensor([-1.4921,  1.1523])\n",
      "Epoch 15900/100000, Frob Loss: 1120.7001391096424, Nuclear Loss: 27.77267837524414, Total loss: 1148.4745716256007,Coefficients_1:tensor([ 1.6575, -0.5753]), Coefficients_2:tensor([0.1575, 0.6819]), Coefficients_3:tensor([-1.4919,  1.1549])\n",
      "Epoch 16000/100000, Frob Loss: 1117.5921464813696, Nuclear Loss: 27.552732467651367, Total loss: 1145.1466220664847,Coefficients_1:tensor([ 1.6570, -0.5738]), Coefficients_2:tensor([0.1580, 0.6823]), Coefficients_3:tensor([-1.4916,  1.1574])\n",
      "Epoch 16100/100000, Frob Loss: 1114.6916159413977, Nuclear Loss: 27.32301139831543, Total loss: 1142.016354793379,Coefficients_1:tensor([ 1.6565, -0.5726]), Coefficients_2:tensor([0.1584, 0.6828]), Coefficients_3:tensor([-1.4914,  1.1605])\n",
      "Epoch 16200/100000, Frob Loss: 1115.2318830873396, Nuclear Loss: 27.216516494750977, Total loss: 1142.4501110416882,Coefficients_1:tensor([ 1.6560, -0.5713]), Coefficients_2:tensor([0.1589, 0.6834]), Coefficients_3:tensor([-1.4912,  1.1638])\n",
      "Epoch 16300/100000, Frob Loss: 1109.9072784291627, Nuclear Loss: 26.774341583251953, Total loss: 1136.6833142933017,Coefficients_1:tensor([ 1.6554, -0.5700]), Coefficients_2:tensor([0.1593, 0.6841]), Coefficients_3:tensor([-1.4909,  1.1667])\n",
      "Epoch 16400/100000, Frob Loss: 1107.8405534371495, Nuclear Loss: 26.512311935424805, Total loss: 1134.354547149641,Coefficients_1:tensor([ 1.6549, -0.5687]), Coefficients_2:tensor([0.1597, 0.6848]), Coefficients_3:tensor([-1.4905,  1.1698])\n",
      "Epoch 16500/100000, Frob Loss: 1106.2485163466197, Nuclear Loss: 26.1912841796875, Total loss: 1132.4414705344832,Coefficients_1:tensor([ 1.6544, -0.5674]), Coefficients_2:tensor([0.1601, 0.6850]), Coefficients_3:tensor([-1.4902,  1.1731])\n",
      "Epoch 16600/100000, Frob Loss: 1105.6305644940614, Nuclear Loss: 25.870656967163086, Total loss: 1131.5028833474528,Coefficients_1:tensor([ 1.6538, -0.5661]), Coefficients_2:tensor([0.1605, 0.6848]), Coefficients_3:tensor([-1.4899,  1.1760])\n",
      "Epoch 16700/100000, Frob Loss: 1102.6076999160703, Nuclear Loss: 25.709304809570312, Total loss: 1128.3186612128754,Coefficients_1:tensor([ 1.6533, -0.5649]), Coefficients_2:tensor([0.1608, 0.6843]), Coefficients_3:tensor([-1.4897,  1.1781])\n",
      "Epoch 16800/100000, Frob Loss: 1101.5426606115957, Nuclear Loss: 25.487117767333984, Total loss: 1127.031427934447,Coefficients_1:tensor([ 1.6527, -0.5636]), Coefficients_2:tensor([0.1612, 0.6835]), Coefficients_3:tensor([-1.4894,  1.1797])\n",
      "Epoch 16900/100000, Frob Loss: 1099.6749978096057, Nuclear Loss: 25.16463851928711, Total loss: 1124.8412804373372,Coefficients_1:tensor([ 1.6522, -0.5623]), Coefficients_2:tensor([0.1615, 0.6827]), Coefficients_3:tensor([-1.4891,  1.1812])\n",
      "Epoch 17000/100000, Frob Loss: 1098.8620498243852, Nuclear Loss: 24.85525894165039, Total loss: 1123.718947392366,Coefficients_1:tensor([ 1.6515, -0.5610]), Coefficients_2:tensor([0.1618, 0.6817]), Coefficients_3:tensor([-1.4888,  1.1827])\n",
      "Epoch 17100/100000, Frob Loss: 1096.8554613472502, Nuclear Loss: 24.669654846191406, Total loss: 1121.5267540237242,Coefficients_1:tensor([ 1.6509, -0.5597]), Coefficients_2:tensor([0.1621, 0.6805]), Coefficients_3:tensor([-1.4886,  1.1845])\n",
      "Epoch 17200/100000, Frob Loss: 1095.4927524343113, Nuclear Loss: 24.417964935302734, Total loss: 1119.9123525734506,Coefficients_1:tensor([ 1.6502, -0.5585]), Coefficients_2:tensor([0.1623, 0.6793]), Coefficients_3:tensor([-1.4884,  1.1867])\n",
      "Epoch 17300/100000, Frob Loss: 1094.1515377840474, Nuclear Loss: 24.14394187927246, Total loss: 1118.2971117559568,Coefficients_1:tensor([ 1.6495, -0.5574]), Coefficients_2:tensor([0.1626, 0.6780]), Coefficients_3:tensor([-1.4883,  1.1880])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17400/100000, Frob Loss: 1092.8406737612395, Nuclear Loss: 23.868078231811523, Total loss: 1116.7103814170996,Coefficients_1:tensor([ 1.6487, -0.5563]), Coefficients_2:tensor([0.1628, 0.6768]), Coefficients_3:tensor([-1.4881,  1.1884])\n",
      "Epoch 17500/100000, Frob Loss: 1091.397972802301, Nuclear Loss: 23.632673263549805, Total loss: 1115.0322730406374,Coefficients_1:tensor([ 1.6479, -0.5554]), Coefficients_2:tensor([0.1630, 0.6756]), Coefficients_3:tensor([-1.4878,  1.1883])\n",
      "Epoch 17600/100000, Frob Loss: 1089.9812915871303, Nuclear Loss: 23.363956451416016, Total loss: 1113.3468714073683,Coefficients_1:tensor([ 1.6469, -0.5546]), Coefficients_2:tensor([0.1632, 0.6745]), Coefficients_3:tensor([-1.4876,  1.1878])\n",
      "Epoch 17700/100000, Frob Loss: 1088.9263146092621, Nuclear Loss: 23.031490325927734, Total loss: 1111.9594271112205,Coefficients_1:tensor([ 1.6458, -0.5540]), Coefficients_2:tensor([0.1634, 0.6735]), Coefficients_3:tensor([-1.4874,  1.1870])\n",
      "Epoch 17800/100000, Frob Loss: 1086.3630346447333, Nuclear Loss: 22.75594139099121, Total loss: 1109.1205938203364,Coefficients_1:tensor([ 1.6443, -0.5538]), Coefficients_2:tensor([0.1635, 0.6725]), Coefficients_3:tensor([-1.4872,  1.1860])\n",
      "Epoch 17900/100000, Frob Loss: 1083.4533454517311, Nuclear Loss: 22.359628677368164, Total loss: 1105.814589013689,Coefficients_1:tensor([ 1.6424, -0.5540]), Coefficients_2:tensor([0.1637, 0.6718]), Coefficients_3:tensor([-1.4871,  1.1848])\n",
      "Epoch 18000/100000, Frob Loss: 1077.003927159048, Nuclear Loss: 21.864587783813477, Total loss: 1098.8701278403582,Coefficients_1:tensor([ 1.6392, -0.5550]), Coefficients_2:tensor([0.1637, 0.6712]), Coefficients_3:tensor([-1.4870,  1.1835])\n",
      "Epoch 18100/100000, Frob Loss: 1067.4191768538815, Nuclear Loss: 21.267446517944336, Total loss: 1088.6882335095809,Coefficients_1:tensor([ 1.6359, -0.5561]), Coefficients_2:tensor([0.1637, 0.6711]), Coefficients_3:tensor([-1.4870,  1.1820])\n",
      "Epoch 18200/100000, Frob Loss: 1063.2923147018575, Nuclear Loss: 21.06308364868164, Total loss: 1084.3569717422636,Coefficients_1:tensor([ 1.6354, -0.5556]), Coefficients_2:tensor([0.1636, 0.6714]), Coefficients_3:tensor([-1.4869,  1.1797])\n",
      "Epoch 18300/100000, Frob Loss: 1061.9304513366715, Nuclear Loss: 20.675397872924805, Total loss: 1082.607333806698,Coefficients_1:tensor([ 1.6353, -0.5563]), Coefficients_2:tensor([0.1636, 0.6715]), Coefficients_3:tensor([-1.4867,  1.1740])\n",
      "Epoch 18400/100000, Frob Loss: 1053.244615615383, Nuclear Loss: 20.493253707885742, Total loss: 1073.7393306496463,Coefficients_1:tensor([ 1.6352, -0.5574]), Coefficients_2:tensor([0.1637, 0.6717]), Coefficients_3:tensor([-1.4858,  1.1740])\n",
      "Epoch 18500/100000, Frob Loss: 1052.0418217022504, Nuclear Loss: 20.315778732299805, Total loss: 1072.3590437630025,Coefficients_1:tensor([ 1.6352, -0.5580]), Coefficients_2:tensor([0.1637, 0.6720]), Coefficients_3:tensor([-1.4851,  1.1734])\n",
      "Epoch 18600/100000, Frob Loss: 1050.8975083529372, Nuclear Loss: 20.146982192993164, Total loss: 1071.0459191660052,Coefficients_1:tensor([ 1.6351, -0.5584]), Coefficients_2:tensor([0.1638, 0.6726]), Coefficients_3:tensor([-1.4845,  1.1729])\n",
      "Epoch 18700/100000, Frob Loss: 1049.5206379468345, Nuclear Loss: 20.006967544555664, Total loss: 1069.5290206092668,Coefficients_1:tensor([ 1.6351, -0.5588]), Coefficients_2:tensor([0.1639, 0.6738]), Coefficients_3:tensor([-1.4840,  1.1723])\n",
      "Epoch 18800/100000, Frob Loss: 1047.643218656677, Nuclear Loss: 19.81460952758789, Total loss: 1067.4592255028208,Coefficients_1:tensor([ 1.6350, -0.5594]), Coefficients_2:tensor([0.1641, 0.6749]), Coefficients_3:tensor([-1.4835,  1.1720])\n",
      "Epoch 18900/100000, Frob Loss: 1045.482345808781, Nuclear Loss: 19.657657623291016, Total loss: 1065.1413830639974,Coefficients_1:tensor([ 1.6349, -0.5603]), Coefficients_2:tensor([0.1642, 0.6758]), Coefficients_3:tensor([-1.4829,  1.1716])\n",
      "Epoch 19000/100000, Frob Loss: 1043.7498328249435, Nuclear Loss: 19.517908096313477, Total loss: 1063.2691075329435,Coefficients_1:tensor([ 1.6348, -0.5617]), Coefficients_2:tensor([0.1643, 0.6768]), Coefficients_3:tensor([-1.4823,  1.1711])\n",
      "Epoch 19100/100000, Frob Loss: 1041.542415862363, Nuclear Loss: 19.410770416259766, Total loss: 1060.9545360602626,Coefficients_1:tensor([ 1.6348, -0.5635]), Coefficients_2:tensor([0.1645, 0.6778]), Coefficients_3:tensor([-1.4815,  1.1710])\n",
      "Epoch 19200/100000, Frob Loss: 1038.8752948032645, Nuclear Loss: 19.274198532104492, Total loss: 1058.1508273847587,Coefficients_1:tensor([ 1.6347, -0.5663]), Coefficients_2:tensor([0.1647, 0.6786]), Coefficients_3:tensor([-1.4803,  1.1717])\n",
      "Epoch 19300/100000, Frob Loss: 1038.1809658203608, Nuclear Loss: 19.08106231689453, Total loss: 1057.2633411073227,Coefficients_1:tensor([ 1.6347, -0.5695]), Coefficients_2:tensor([0.1649, 0.6795]), Coefficients_3:tensor([-1.4788,  1.1717])\n",
      "Epoch 19400/100000, Frob Loss: 1034.5575632027387, Nuclear Loss: 18.946069717407227, Total loss: 1053.504926707413,Coefficients_1:tensor([ 1.6346, -0.5727]), Coefficients_2:tensor([0.1651, 0.6805]), Coefficients_3:tensor([-1.4775,  1.1717])\n",
      "Epoch 19500/100000, Frob Loss: 1033.8425105797864, Nuclear Loss: 18.862503051757812, Total loss: 1052.7062916693317,Coefficients_1:tensor([ 1.6346, -0.5760]), Coefficients_2:tensor([0.1653, 0.6815]), Coefficients_3:tensor([-1.4772,  1.1705])\n",
      "Epoch 19600/100000, Frob Loss: 1033.3714231739866, Nuclear Loss: 18.58390235900879, Total loss: 1051.9565901521707,Coefficients_1:tensor([ 1.6345, -0.5794]), Coefficients_2:tensor([0.1655, 0.6826]), Coefficients_3:tensor([-1.4772,  1.1673])\n",
      "Epoch 19700/100000, Frob Loss: 1028.809306318245, Nuclear Loss: 18.45296287536621, Total loss: 1047.2635240000227,Coefficients_1:tensor([ 1.6345, -0.5832]), Coefficients_2:tensor([0.1658, 0.6837]), Coefficients_3:tensor([-1.4774,  1.1621])\n",
      "Epoch 19800/100000, Frob Loss: 1025.927331324101, Nuclear Loss: 18.279165267944336, Total loss: 1044.2077388761263,Coefficients_1:tensor([ 1.6345, -0.5876]), Coefficients_2:tensor([0.1660, 0.6847]), Coefficients_3:tensor([-1.4778,  1.1562])\n",
      "Epoch 19900/100000, Frob Loss: 1022.1370397124596, Nuclear Loss: 18.21096420288086, Total loss: 1040.349239911015,Coefficients_1:tensor([ 1.6345, -0.5922]), Coefficients_2:tensor([0.1663, 0.6856]), Coefficients_3:tensor([-1.4782,  1.1496])\n",
      "Epoch 20000/100000, Frob Loss: 1018.2872358287003, Nuclear Loss: 18.09025764465332, Total loss: 1036.3787213453343,Coefficients_1:tensor([ 1.6344, -0.5967]), Coefficients_2:tensor([0.1665, 0.6861]), Coefficients_3:tensor([-1.4790,  1.1399])\n",
      "Epoch 20100/100000, Frob Loss: 1019.7434382101087, Nuclear Loss: 18.180044174194336, Total loss: 1037.9246786029576,Coefficients_1:tensor([ 1.6344, -0.6016]), Coefficients_2:tensor([0.1668, 0.6862]), Coefficients_3:tensor([-1.4787,  1.1373])\n",
      "Epoch 20200/100000, Frob Loss: 1017.504532694848, Nuclear Loss: 17.900358200073242, Total loss: 1035.4060653717338,Coefficients_1:tensor([ 1.6344, -0.6065]), Coefficients_2:tensor([0.1670, 0.6862]), Coefficients_3:tensor([-1.4784,  1.1336])\n",
      "Epoch 20300/100000, Frob Loss: 1009.4812838476532, Nuclear Loss: 17.89928436279297, Total loss: 1027.3817245310088,Coefficients_1:tensor([ 1.6343, -0.6114]), Coefficients_2:tensor([0.1672, 0.6858]), Coefficients_3:tensor([-1.4781,  1.1288])\n",
      "Epoch 20400/100000, Frob Loss: 1007.0351684376068, Nuclear Loss: 17.79939842224121, Total loss: 1024.8357059439581,Coefficients_1:tensor([ 1.6343, -0.6163]), Coefficients_2:tensor([0.1675, 0.6853]), Coefficients_3:tensor([-1.4780,  1.1231])\n",
      "Epoch 20500/100000, Frob Loss: 1008.7717596350813, Nuclear Loss: 17.846891403198242, Total loss: 1026.6197745850186,Coefficients_1:tensor([ 1.6343, -0.6212]), Coefficients_2:tensor([0.1677, 0.6846]), Coefficients_3:tensor([-1.4780,  1.1162])\n",
      "Epoch 20600/100000, Frob Loss: 1002.5430356217506, Nuclear Loss: 17.595914840698242, Total loss: 1020.140059258901,Coefficients_1:tensor([ 1.6342, -0.6261]), Coefficients_2:tensor([0.1679, 0.6837]), Coefficients_3:tensor([-1.4782,  1.1085])\n",
      "Epoch 20700/100000, Frob Loss: 999.9557117209422, Nuclear Loss: 17.466257095336914, Total loss: 1017.4230650576881,Coefficients_1:tensor([ 1.6342, -0.6311]), Coefficients_2:tensor([0.1682, 0.6827]), Coefficients_3:tensor([-1.4783,  1.1010])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20800/100000, Frob Loss: 997.9118654467754, Nuclear Loss: 17.401649475097656, Total loss: 1015.3146006756585,Coefficients_1:tensor([ 1.6342, -0.6361]), Coefficients_2:tensor([0.1684, 0.6817]), Coefficients_3:tensor([-1.4783,  1.0936])\n",
      "Epoch 20900/100000, Frob Loss: 995.5775482083458, Nuclear Loss: 17.319015502929688, Total loss: 1012.8976388832574,Coefficients_1:tensor([ 1.6341, -0.6414]), Coefficients_2:tensor([0.1686, 0.6806]), Coefficients_3:tensor([-1.4782,  1.0863])\n",
      "Epoch 21000/100000, Frob Loss: 995.470908502744, Nuclear Loss: 17.198060989379883, Total loss: 1012.6700371263302,Coefficients_1:tensor([ 1.6341, -0.6466]), Coefficients_2:tensor([0.1688, 0.6795]), Coefficients_3:tensor([-1.4779,  1.0794])\n",
      "Epoch 21100/100000, Frob Loss: 992.1395027033027, Nuclear Loss: 17.193517684936523, Total loss: 1009.3340817010935,Coefficients_1:tensor([ 1.6341, -0.6518]), Coefficients_2:tensor([0.1690, 0.6784]), Coefficients_3:tensor([-1.4777,  1.0725])\n",
      "Epoch 21200/100000, Frob Loss: 989.7394318440067, Nuclear Loss: 17.21681785583496, Total loss: 1006.9573068042821,Coefficients_1:tensor([ 1.6341, -0.6569]), Coefficients_2:tensor([0.1692, 0.6774]), Coefficients_3:tensor([-1.4773,  1.0662])\n",
      "Epoch 21300/100000, Frob Loss: 988.1376250149679, Nuclear Loss: 17.182676315307617, Total loss: 1005.3213529892728,Coefficients_1:tensor([ 1.6341, -0.6620]), Coefficients_2:tensor([0.1694, 0.6764]), Coefficients_3:tensor([-1.4770,  1.0607])\n",
      "Epoch 21400/100000, Frob Loss: 986.4471498672845, Nuclear Loss: 17.168577194213867, Total loss: 1003.6167739653954,Coefficients_1:tensor([ 1.6341, -0.6670]), Coefficients_2:tensor([0.1696, 0.6754]), Coefficients_3:tensor([-1.4766,  1.0554])\n",
      "Epoch 21500/100000, Frob Loss: 984.8032056757464, Nuclear Loss: 17.149642944335938, Total loss: 1001.9538894765527,Coefficients_1:tensor([ 1.6341, -0.6720]), Coefficients_2:tensor([0.1697, 0.6744]), Coefficients_3:tensor([-1.4763,  1.0506])\n",
      "Epoch 21600/100000, Frob Loss: 983.0376143832525, Nuclear Loss: 17.1278018951416, Total loss: 1000.1664509184027,Coefficients_1:tensor([ 1.6340, -0.6769]), Coefficients_2:tensor([0.1699, 0.6731]), Coefficients_3:tensor([-1.4760,  1.0459])\n",
      "Epoch 21700/100000, Frob Loss: 979.8140581486791, Nuclear Loss: 17.16217613220215, Total loss: 996.9772622470323,Coefficients_1:tensor([ 1.6340, -0.6819]), Coefficients_2:tensor([0.1700, 0.6716]), Coefficients_3:tensor([-1.4757,  1.0412])\n",
      "Epoch 21800/100000, Frob Loss: 977.0001203818099, Nuclear Loss: 17.200927734375, Total loss: 994.2020738166609,Coefficients_1:tensor([ 1.6340, -0.6865]), Coefficients_2:tensor([0.1701, 0.6696]), Coefficients_3:tensor([-1.4754,  1.0357])\n",
      "Epoch 21900/100000, Frob Loss: 974.4574847766073, Nuclear Loss: 17.252513885498047, Total loss: 991.7110211925922,Coefficients_1:tensor([ 1.6339, -0.6910]), Coefficients_2:tensor([0.1702, 0.6671]), Coefficients_3:tensor([-1.4754,  1.0286])\n",
      "Epoch 22000/100000, Frob Loss: 972.1555498764462, Nuclear Loss: 17.292037963867188, Total loss: 989.448606167625,Coefficients_1:tensor([ 1.6339, -0.6955]), Coefficients_2:tensor([0.1702, 0.6641]), Coefficients_3:tensor([-1.4755,  1.0201])\n",
      "Epoch 22100/100000, Frob Loss: 969.2624360039289, Nuclear Loss: 17.29526710510254, Total loss: 986.5587139247608,Coefficients_1:tensor([ 1.6338, -0.7000]), Coefficients_2:tensor([0.1702, 0.6608]), Coefficients_3:tensor([-1.4756,  1.0103])\n",
      "Epoch 22200/100000, Frob Loss: 966.305337062069, Nuclear Loss: 17.312698364257812, Total loss: 983.6190389545735,Coefficients_1:tensor([ 1.6338, -0.7046]), Coefficients_2:tensor([0.1702, 0.6578]), Coefficients_3:tensor([-1.4758,  1.0001])\n",
      "Epoch 22300/100000, Frob Loss: 962.1542383864238, Nuclear Loss: 17.403478622436523, Total loss: 979.5587123633544,Coefficients_1:tensor([ 1.6338, -0.7094]), Coefficients_2:tensor([0.1702, 0.6547]), Coefficients_3:tensor([-1.4758,  0.9915])\n",
      "Epoch 22400/100000, Frob Loss: 953.5324994823144, Nuclear Loss: 17.56505012512207, Total loss: 971.0985375304421,Coefficients_1:tensor([ 1.6338, -0.7148]), Coefficients_2:tensor([0.1701, 0.6515]), Coefficients_3:tensor([-1.4753,  0.9847])\n",
      "Epoch 22500/100000, Frob Loss: 930.7504481246674, Nuclear Loss: 18.324718475341797, Total loss: 949.0761550098637,Coefficients_1:tensor([ 1.6339, -0.7211]), Coefficients_2:tensor([0.1701, 0.6488]), Coefficients_3:tensor([-1.4724,  0.9753])\n",
      "Epoch 22600/100000, Frob Loss: 915.3817117241441, Nuclear Loss: 18.309560775756836, Total loss: 933.6922466356071,Coefficients_1:tensor([ 1.6336, -0.7266]), Coefficients_2:tensor([0.1698, 0.6475]), Coefficients_3:tensor([-1.4679,  0.9669])\n",
      "Epoch 22700/100000, Frob Loss: 910.9712788115038, Nuclear Loss: 18.11104965209961, Total loss: 929.083283146775,Coefficients_1:tensor([ 1.6335, -0.7294]), Coefficients_2:tensor([0.1698, 0.6478]), Coefficients_3:tensor([-1.4654,  0.9528])\n",
      "Epoch 22800/100000, Frob Loss: 907.9509577101013, Nuclear Loss: 17.937841415405273, Total loss: 925.8897335830876,Coefficients_1:tensor([ 1.6336, -0.7320]), Coefficients_2:tensor([0.1698, 0.6479]), Coefficients_3:tensor([-1.4603,  0.9417])\n",
      "Epoch 22900/100000, Frob Loss: 909.9772349272423, Nuclear Loss: 17.69707679748535, Total loss: 927.6752271569019,Coefficients_1:tensor([ 1.6336, -0.7344]), Coefficients_2:tensor([0.1698, 0.6480]), Coefficients_3:tensor([-1.4556,  0.9299])\n",
      "Epoch 23000/100000, Frob Loss: 903.0796346039207, Nuclear Loss: 17.645978927612305, Total loss: 920.7265128486252,Coefficients_1:tensor([ 1.6336, -0.7368]), Coefficients_2:tensor([0.1698, 0.6479]), Coefficients_3:tensor([-1.4511,  0.9236])\n",
      "Epoch 23100/100000, Frob Loss: 901.5984804236213, Nuclear Loss: 17.56318473815918, Total loss: 919.1625462322844,Coefficients_1:tensor([ 1.6336, -0.7394]), Coefficients_2:tensor([0.1698, 0.6478]), Coefficients_3:tensor([-1.4443,  0.9317])\n",
      "Epoch 23200/100000, Frob Loss: 900.1986731190615, Nuclear Loss: 17.332286834716797, Total loss: 917.5318280556154,Coefficients_1:tensor([ 1.6336, -0.7420]), Coefficients_2:tensor([0.1698, 0.6476]), Coefficients_3:tensor([-1.4384,  0.9397])\n",
      "Epoch 23300/100000, Frob Loss: 897.3901373823836, Nuclear Loss: 17.305377960205078, Total loss: 914.6963714126695,Coefficients_1:tensor([ 1.6336, -0.7445]), Coefficients_2:tensor([0.1698, 0.6473]), Coefficients_3:tensor([-1.4348,  0.9438])\n",
      "Epoch 23400/100000, Frob Loss: 895.7908119505473, Nuclear Loss: 17.214475631713867, Total loss: 913.0061309190087,Coefficients_1:tensor([ 1.6335, -0.7472]), Coefficients_2:tensor([0.1699, 0.6468]), Coefficients_3:tensor([-1.4322,  0.9471])\n",
      "Epoch 23500/100000, Frob Loss: 893.999118377402, Nuclear Loss: 17.14031219482422, Total loss: 911.1402649366121,Coefficients_1:tensor([ 1.6335, -0.7499]), Coefficients_2:tensor([0.1699, 0.6460]), Coefficients_3:tensor([-1.4300,  0.9455])\n",
      "Epoch 23600/100000, Frob Loss: 893.1003266242261, Nuclear Loss: 17.17191505432129, Total loss: 910.2730692315309,Coefficients_1:tensor([ 1.6334, -0.7528]), Coefficients_2:tensor([0.1699, 0.6449]), Coefficients_3:tensor([-1.4292,  0.9408])\n",
      "Epoch 23700/100000, Frob Loss: 890.715280140116, Nuclear Loss: 17.00817108154297, Total loss: 907.7242791609667,Coefficients_1:tensor([ 1.6333, -0.7557]), Coefficients_2:tensor([0.1699, 0.6437]), Coefficients_3:tensor([-1.4289,  0.9413])\n",
      "Epoch 23800/100000, Frob Loss: 889.352915133603, Nuclear Loss: 16.93050765991211, Total loss: 906.2842516502338,Coefficients_1:tensor([ 1.6332, -0.7587]), Coefficients_2:tensor([0.1699, 0.6423]), Coefficients_3:tensor([-1.4289,  0.9375])\n",
      "Epoch 23900/100000, Frob Loss: 887.728884797761, Nuclear Loss: 16.902978897094727, Total loss: 904.6326984956826,Coefficients_1:tensor([ 1.6331, -0.7617]), Coefficients_2:tensor([0.1700, 0.6408]), Coefficients_3:tensor([-1.4276,  0.9288])\n",
      "Epoch 24000/100000, Frob Loss: 886.2666619967556, Nuclear Loss: 16.90193748474121, Total loss: 903.1694508380958,Coefficients_1:tensor([ 1.6329, -0.7645]), Coefficients_2:tensor([0.1700, 0.6392]), Coefficients_3:tensor([-1.4261,  0.9259])\n",
      "Epoch 24100/100000, Frob Loss: 886.8832263155887, Nuclear Loss: 17.06826400756836, Total loss: 903.9523541148333,Coefficients_1:tensor([ 1.6328, -0.7675]), Coefficients_2:tensor([0.1701, 0.6379]), Coefficients_3:tensor([-1.4265,  0.9218])\n",
      "Epoch 24200/100000, Frob Loss: 883.5569046294222, Nuclear Loss: 16.882049560546875, Total loss: 900.4398313419915,Coefficients_1:tensor([ 1.6326, -0.7704]), Coefficients_2:tensor([0.1702, 0.6368]), Coefficients_3:tensor([-1.4274,  0.9164])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24300/100000, Frob Loss: 883.2019274240324, Nuclear Loss: 16.8165340423584, Total loss: 900.0193495971348,Coefficients_1:tensor([ 1.6324, -0.7734]), Coefficients_2:tensor([0.1703, 0.6355]), Coefficients_3:tensor([-1.4291,  0.9109])\n",
      "Epoch 24400/100000, Frob Loss: 881.017669571086, Nuclear Loss: 16.90079116821289, Total loss: 897.9193547108906,Coefficients_1:tensor([ 1.6323, -0.7764]), Coefficients_2:tensor([0.1703, 0.6344]), Coefficients_3:tensor([-1.4302,  0.9084])\n",
      "Epoch 24500/100000, Frob Loss: 879.7707764895919, Nuclear Loss: 16.936752319335938, Total loss: 896.7084286822525,Coefficients_1:tensor([ 1.6321, -0.7795]), Coefficients_2:tensor([0.1704, 0.6334]), Coefficients_3:tensor([-1.4308,  0.9064])\n",
      "Epoch 24600/100000, Frob Loss: 878.6762211371636, Nuclear Loss: 16.88997459411621, Total loss: 895.5671069865305,Coefficients_1:tensor([ 1.6319, -0.7826]), Coefficients_2:tensor([0.1705, 0.6324]), Coefficients_3:tensor([-1.4308,  0.9099])\n",
      "Epoch 24700/100000, Frob Loss: 877.3349506085709, Nuclear Loss: 16.922517776489258, Total loss: 894.2583854530443,Coefficients_1:tensor([ 1.6317, -0.7856]), Coefficients_2:tensor([0.1707, 0.6316]), Coefficients_3:tensor([-1.4306,  0.9238])\n",
      "Epoch 24800/100000, Frob Loss: 883.4663514279754, Nuclear Loss: 17.13996696472168, Total loss: 900.6072447861305,Coefficients_1:tensor([ 1.6315, -0.7887]), Coefficients_2:tensor([0.1708, 0.6310]), Coefficients_3:tensor([-1.4305,  0.9425])\n",
      "Epoch 24900/100000, Frob Loss: 874.9786463697495, Nuclear Loss: 16.931289672851562, Total loss: 891.9108710154945,Coefficients_1:tensor([ 1.6313, -0.7916]), Coefficients_2:tensor([0.1709, 0.6304]), Coefficients_3:tensor([-1.4299,  0.9621])\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100000\n",
    "lambda_k = 0.1\n",
    "lambda_TV = 0.5\n",
    "\n",
    "for epoch in range(num_epochs + 1):\n",
    "    x_NN, t_NN = inputs_tensor[:, 0:1], inputs_tensor[:, 1:2]\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    f1_full, f2_full, f3_full, f1_full_nos, f2_full_nos, f3_full_nos = model(x_NN,t_NN)\n",
    "    \n",
    "    frobenius_loss = torch.norm(Q - f1_full.view(Nx, Nt) - f2_full.view(Nx, Nt) - f3_full.view(Nx, Nt),  'fro') ** 2\n",
    "    \n",
    "    nuclear_loss_q1 = NuclearNormAutograd.apply(f1_full_nos.view(Nx, Nt)) \n",
    "    nuclear_loss_q2 = NuclearNormAutograd.apply(f2_full_nos.view(Nx, Nt))\n",
    "    nuclear_loss_q3 = NuclearNormAutograd.apply(f3_full_nos.view(Nx, Nt))\n",
    "    nuclear_loss = lambda_k * (nuclear_loss_q1 + nuclear_loss_q2 + nuclear_loss_q3)\n",
    "    \n",
    "    TV_loss = lambda_TV * (TV(f1_full_nos.view(Nx, Nt), Nx, Nt) + TV(f2_full_nos.view(Nx, Nt), Nx, Nt) + TV(f3_full_nos.view(Nx, Nt), Nx, Nt))\n",
    "    \n",
    "    total_loss = nuclear_loss + frobenius_loss + TV_loss\n",
    "    \n",
    "    total_loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "    \n",
    "    shift_coeffs1 = torch.tensor([p.item() for p in model.alphas1])\n",
    "    shift_coeffs2 = torch.tensor([p.item() for p in model.alphas2])\n",
    "    shift_coeffs3 = torch.tensor([p.item() for p in model.alphas3])\n",
    "    \n",
    "    if frobenius_loss < 1.0:\n",
    "        print(\"Early stopping is triggered\")\n",
    "        break\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(\n",
    "            f'Epoch {epoch}/{num_epochs}, Frob Loss: {frobenius_loss.item()}, Nuclear Loss: {nuclear_loss.item()}, Total loss: {total_loss.item()},'\n",
    "            f'Coefficients_1:{shift_coeffs1}, Coefficients_2:{shift_coeffs2}, Coefficients_3:{shift_coeffs3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f62484c-890c-4ca4-9b1b-d99a33eedc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = f1_full + f2_full + f3_full\n",
    "Q_tilde = combined.view(Nx, Nt).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81056b0-f351-422d-bf19-64934731a933",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 7, figsize=(16, 4))\n",
    "vmin = np.min(Q_tilde)\n",
    "vmax = np.max(Q_tilde)\n",
    "\n",
    "#Qtilde\n",
    "axs[0].pcolormesh(xx.T, tt.T, Q_tilde, vmin=vmin, vmax=vmax)\n",
    "axs[0].set_title(r\"$\\mathbf{\\tilde{Q}}$\")\n",
    "axs[0].set_xlabel(\"t\")\n",
    "axs[0].set_ylabel(\"x\")\n",
    "axs[0].set_xticks([])\n",
    "axs[0].set_yticks([])\n",
    "\n",
    "# f^1\n",
    "axs[1].pcolormesh(xx.T, tt.T, f1_full.view(Nx, Nt).detach().numpy(), vmin=vmin, vmax=vmax)\n",
    "axs[1].set_title(r\"$\\mathcal{T}^1\\mathbf{Q}^1$\")\n",
    "axs[1].set_xlabel(\"t\")\n",
    "axs[1].set_ylabel(\"x\")\n",
    "axs[1].set_xticks([])\n",
    "axs[1].set_yticks([])\n",
    "\n",
    "# f^3\n",
    "axs[2].pcolormesh(xx.T, tt.T, f2_full.view(Nx,Nt).detach().numpy(), vmin=vmin, vmax=vmax)\n",
    "axs[2].set_title(r\"$\\mathcal{T}^2\\mathbf{Q}^2$\")\n",
    "axs[2].set_xlabel(\"t\")\n",
    "axs[2].set_ylabel(\"x\")\n",
    "axs[2].set_xticks([])\n",
    "axs[2].set_yticks([])\n",
    "\n",
    "# f^2\n",
    "axs[3].pcolormesh(xx.T, tt.T, f3_full.view(Nx,Nt).detach().numpy(), vmin=vmin, vmax=vmax)\n",
    "axs[3].set_title(r\"$\\mathcal{T}^3\\mathbf{Q}^3$\")\n",
    "axs[3].set_xlabel(\"t\")\n",
    "axs[3].set_ylabel(\"x\")\n",
    "axs[3].set_xticks([])\n",
    "axs[3].set_yticks([])\n",
    "\n",
    "\n",
    "# f^1\n",
    "axs[4].pcolormesh(xx.T, tt.T, f1_full_nos.view(Nx,Nt).detach().numpy(), vmin=vmin, vmax=vmax)\n",
    "axs[4].set_title(r\"$\\mathbf{Q}^1$\")\n",
    "axs[4].set_xlabel(\"t\")\n",
    "axs[4].set_ylabel(\"x\")\n",
    "axs[4].set_xticks([])\n",
    "axs[4].set_yticks([])\n",
    "\n",
    "# f^3\n",
    "axs[5].pcolormesh(xx.T, tt.T, f2_full_nos.view(Nx,Nt).detach().numpy(), vmin=vmin, vmax=vmax)\n",
    "axs[5].set_title(r\"$\\mathbf{Q}^2$\")\n",
    "axs[5].set_xlabel(\"t\")\n",
    "axs[5].set_ylabel(\"x\")\n",
    "axs[5].set_xticks([])\n",
    "axs[5].set_yticks([])\n",
    "\n",
    "# f^2\n",
    "cax4 = axs[6].pcolormesh(xx.T, tt.T, f3_full_nos.view(Nx,Nt).detach().numpy(), vmin=vmin, vmax=vmax)\n",
    "axs[6].set_title(r\"$\\mathbf{Q}^3$\")\n",
    "axs[6].set_xlabel(\"t\")\n",
    "axs[6].set_ylabel(\"x\")\n",
    "axs[6].set_xticks([])\n",
    "axs[6].set_yticks([])\n",
    "\n",
    "plt.colorbar(cax4, ax=axs.ravel().tolist(), orientation='vertical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c2a629",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_fig(filepath=immpath + \"Wildfire_NN\", figure=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9af982",
   "metadata": {},
   "source": [
    "## Saving the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e2f610",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), impath + 'Wildfire.pth')\n",
    "np.save(impath + 'Q.npy', Q)\n",
    "np.save(impath + 'Q_tilde.npy', Q_tilde)\n",
    "np.save(impath + 'T1Q1.npy', f1_full.view(Nx, Nt).detach().numpy())\n",
    "np.save(impath + 'T3Q3.npy', f2_full.view(Nx, Nt).detach().numpy())\n",
    "np.save(impath + 'T2Q2.npy', f3_full.view(Nx, Nt).detach().numpy())\n",
    "np.save(impath + 'Q1.npy', f1_full_nos.view(Nx, Nt).detach().numpy())\n",
    "np.save(impath + 'Q3.npy', f2_full_nos.view(Nx, Nt).detach().numpy())\n",
    "np.save(impath + 'Q2.npy', f3_full_nos.view(Nx, Nt).detach().numpy())\n",
    "np.save(impath + 'shifts1.npy', shift_coeffs1.detach().numpy()) \n",
    "np.save(impath + 'shifts2.npy', shift_coeffs2.detach().numpy()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6fa383",
   "metadata": {},
   "source": [
    "# Apply rsPOD taking the results from the above as initial guesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf8dc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"./sPOD/lib/\")\n",
    "\n",
    "import numpy as np\n",
    "from numpy import meshgrid\n",
    "import matplotlib.pyplot as plt\n",
    "from sPOD_algo import (\n",
    "    shifted_POD,\n",
    "    sPOD_Param,\n",
    "    give_interpolation_error,\n",
    ")\n",
    "from transforms import Transform\n",
    "from plot_utils import save_fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dfe17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "Q = np.load(impath + 'Q.npy')\n",
    "Q_tilde = np.load(impath + 'Q_tilde.npy')\n",
    "T1Q1 = np.load(impath + 'T1Q1.npy')\n",
    "T2Q2 = np.load(impath + 'T2Q2.npy')\n",
    "T3Q3 = np.load(impath + 'T3Q3.npy')\n",
    "Q1 = np.load(impath + 'Q1.npy')\n",
    "Q2 = np.load(impath + 'Q2.npy')\n",
    "Q3 = np.load(impath + 'Q3.npy')\n",
    "shifts1 = np.load(impath + 'shifts1.npy')\n",
    "shifts2 = np.load(impath + 'shifts2.npy') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c8dbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "err = np.linalg.norm(Q - Q_tilde) / np.linalg.norm(Q)\n",
    "print(\"NN prediction error: %1.2e \" % err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462cb012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the transformations\n",
    "L = x[-1]\n",
    "dx = x[1] - x[0]\n",
    "s1 = np.polyval(shifts1, t)\n",
    "s2 = np.zeros_like(s1)\n",
    "s3 = np.polyval(shifts2, t)\n",
    "\n",
    "data_shape = [Nx, 1, 1, Nt]\n",
    "transfos = [\n",
    "    Transform(data_shape, [L], shifts=s1, dx=[dx], interp_order=5),\n",
    "    Transform(data_shape, [L], shifts=s2, dx=[dx], interp_order=5),\n",
    "    Transform(data_shape, [L], shifts=s3, dx=[dx], interp_order=5),\n",
    "]\n",
    "\n",
    "interp_err = np.max([give_interpolation_error(Q, trafo) for trafo in transfos])\n",
    "print(\"interpolation error: %1.2e \" % interp_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb54843",
   "metadata": {},
   "outputs": [],
   "source": [
    "METHOD = \"ALM\"\n",
    "\n",
    "# Parameters\n",
    "mu0 = Nx * Nt / (4 * np.sum(np.abs(Q)))\n",
    "lambd0 = 4000\n",
    "myparams = sPOD_Param()\n",
    "myparams.maxit = 100\n",
    "param_alm = mu0 * 0.01 # adjust for case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67977c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the ALM method\n",
    "ret = shifted_POD(Q, transfos, [1, 1, 1], myparams, METHOD, param_alm, [T1Q1, T2Q2, T3Q3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba91cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sPOD_frames, qtilde, rel_err = ret.frames, ret.data_approx, ret.rel_err_hist\n",
    "qf = [\n",
    "    np.squeeze(np.reshape(trafo.apply(frame.build_field()), data_shape))\n",
    "    for trafo, frame in zip(transfos, ret.frames)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873db470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% 1. visualize your results: sPOD frames\n",
    "############################################\n",
    "\n",
    "fig, axs = plt.subplots(1, 5, figsize=(25, 6))\n",
    "vmin = np.min(Q)\n",
    "vmax = np.max(Q)\n",
    "\n",
    "axs[0].pcolormesh(xx.T, tt.T, Q, vmin=vmin, vmax=vmax)\n",
    "axs[0].set_title(r\"${\\mathbf{Q}}$\")\n",
    "axs[0].set_xlabel(\"x\")\n",
    "axs[0].set_ylabel(\"t\")\n",
    "axs[0].set_xticks([])\n",
    "axs[0].set_yticks([])\n",
    "\n",
    "#Qtilde\n",
    "axs[1].pcolormesh(xx.T, tt.T, qtilde, vmin=vmin, vmax=vmax)\n",
    "axs[1].set_title(r\"$\\tilde{\\mathbf{Q}}$\")\n",
    "axs[1].set_xlabel(\"x\")\n",
    "axs[1].set_ylabel(\"t\")\n",
    "axs[1].set_xticks([])\n",
    "axs[1].set_yticks([])\n",
    "\n",
    "# f^1\n",
    "axs[2].pcolormesh(xx.T, tt.T, qf[0], vmin=vmin, vmax=vmax)\n",
    "axs[2].set_title(r\"$\\mathcal{T}^1\\mathbf{Q}^1$\")\n",
    "axs[2].set_xlabel(\"x\")\n",
    "axs[2].set_ylabel(\"t\")\n",
    "axs[2].set_xticks([])\n",
    "axs[2].set_yticks([])\n",
    "\n",
    "# f^2\n",
    "axs[3].pcolormesh(xx.T, tt.T, qf[1], vmin=vmin, vmax=vmax)\n",
    "axs[3].set_title(r\"$\\mathcal{T}^2\\mathbf{Q}^2$\")\n",
    "axs[3].set_xlabel(\"x\")\n",
    "axs[3].set_ylabel(\"t\")\n",
    "axs[3].set_xticks([])\n",
    "axs[3].set_yticks([])\n",
    "\n",
    "# f^3\n",
    "cax4 = axs[4].pcolormesh(xx.T, tt.T, qf[2], vmin=vmin, vmax=vmax)\n",
    "axs[4].set_title(r\"$\\mathcal{T}^3\\mathbf{Q}^3$\")\n",
    "axs[4].set_xlabel(\"x\")\n",
    "axs[4].set_ylabel(\"t\")\n",
    "axs[4].set_xticks([])\n",
    "axs[4].set_yticks([])\n",
    "\n",
    "plt.colorbar(cax4, ax=axs.ravel().tolist(), orientation='vertical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d01ad9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_fig(filepath=immpath + \"Wildfire_sPOD\", figure=fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
